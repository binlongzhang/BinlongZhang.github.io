<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico">
  <link rel="mask-icon" href="/images/logo.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"binlongzhang.github.io","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":"ture","color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="引言 支持向量机（support vector machines,SVM）是一种二分类模型；它的基本模型定义在特征空间上的间隔最大模型分类器，间隔最大使之区别于感知机；同时还可以使用核技巧，使它成为非线性支持向量机；SVM的学习策略就是间隔最大化，等价于正则化的合页函数最小化；支持向量机的学习算法就是求解凸二次规划的最优化算法；">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机">
<meta property="og:url" content="https://binlongzhang.github.io/280b588e/index.html">
<meta property="og:site_name" content="Gonlnib&#39;s Blog">
<meta property="og:description" content="引言 支持向量机（support vector machines,SVM）是一种二分类模型；它的基本模型定义在特征空间上的间隔最大模型分类器，间隔最大使之区别于感知机；同时还可以使用核技巧，使它成为非线性支持向量机；SVM的学习策略就是间隔最大化，等价于正则化的合页函数最小化；支持向量机的学习算法就是求解凸二次规划的最优化算法；">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%AF%81%E6%98%8E%E4%BB%8Ea%E6%B1%82%E8%A7%A3w,b.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%AF%81%E6%98%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%B1%82w,b.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%AF%81%E6%98%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%B1%82w,b%E4%B8%8B.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%951.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%952.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%BD%AF%E9%97%B4%E9%9A%94%E7%9A%84%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E6%A0%B8%E6%8A%80%E5%B7%A7%E7%A4%BA%E4%BE%8B.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%AF%81%E6%98%8E%E5%AE%9A%E4%B9%89%E8%BF%90%E7%AE%97%E6%98%AFs%E7%9A%84%E5%86%85%E7%A7%AF_1.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E8%AF%81%E6%98%8E%E5%AE%9A%E4%B9%89%E8%BF%90%E7%AE%97%E6%98%AFs%E7%9A%84%E5%86%85%E7%A7%AF_2.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4S%E5%AE%8C%E5%A4%87%E5%8C%96.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0_1.png">
<meta property="og:image" content="https://binlongzhang.github.io/280b588e/%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0_2.png">
<meta property="article:published_time" content="2020-02-10T11:17:28.000Z">
<meta property="article:modified_time" content="2020-02-26T15:11:05.385Z">
<meta property="article:author" content="Gonlnib">
<meta property="article:tag" content="攻克&#39;小蓝书&#39;">
<meta property="article:tag" content="原理">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="支持向量机">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://binlongzhang.github.io/280b588e/%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94.png">

<link rel="canonical" href="https://binlongzhang.github.io/280b588e/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>支持向量机 | Gonlnib's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Gonlnib's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">2</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">6</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://binlongzhang.github.io/280b588e/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="Gonlnib">
      <meta itemprop="description" content="Gonlnib 的个人博客，主要内容会涉及一些学习记录、思考、理解，也会有一些个人爱好的展示">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gonlnib's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          支持向量机
        </h1>

        <div class="post-meta">

          
          	<i class="fa fa-thumb-tack"></i>
          	<font color=7D26CD>置顶</font>
          	<span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-10 19:17:28" itemprop="dateCreated datePublished" datetime="2020-02-10T19:17:28+08:00">2020-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-26 23:11:05" itemprop="dateModified" datetime="2020-02-26T23:11:05+08:00">2020-02-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">统计学习方法</span>
                  </a>
                </span>
            </span>

          
          <br>
            <span id="/280b588e/" class="post-meta-item leancloud_visitors" data-flag-title="支持向量机" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/280b588e/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/280b588e/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="引言">引言</h1>
<p>支持向量机（support vector machines,SVM）是一种二分类模型；它的基本模型定义在特征空间上的间隔最大模型分类器，间隔最大使之区别于感知机；同时还可以使用核技巧，使它成为非线性支持向量机；SVM的学习策略就是间隔最大化，等价于正则化的合页函数最小化；支持向量机的学习算法就是求解凸二次规划的最优化算法；</p>
<p>由简至繁的模型分别为</p>
<ul>
<li>线性可分支持向量机--硬间隔最大化</li>
<li>线性支持向量机--软间隔最大化</li>
<li>非线性支持向量机--核技巧+软间隔最大化</li>
</ul>
<h1 id="线性可分支持向量机">线性可分支持向量机</h1>
<p>线性可分支持向量机、线性支持向量机假设输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；支持向量机的学习实在特征空间内进行的；</p>
<p>假设训练数据集是线性可分的，且对于样本点<span class="math inline">\((x_i,y_i)\)</span>,当<span class="math inline">\(y_i=+1\)</span>时<span class="math inline">\(x_i\)</span>为正例；当<span class="math inline">\(y_i=-1\)</span>时<span class="math inline">\(x_i\)</span>为负例；</p>
<p>学习的目标是找到分离超平面<span class="math inline">\(w\cdot x+b=0,\quad w\)</span>是法向量，<span class="math inline">\(b\)</span>为截距，法向量指向的是正类；一般的，训练数据集线性可分时存在无穷多个分离超平面将两类数据正确划分。感知机利用误分类最小策略，求得分离超平面；线性可分支持向量机利用间隔最大化求最优分离超平面；</p>
<p><strong>定义</strong>：（线性可分支持向量机）给定线性可分数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为 <span class="math display">\[
w^*\cdot x+b^*=0
\]</span> 相应的决策函数为 <span class="math display">\[
f(x)=sign(w^*\cdot x+b^*)
\]</span></p>
<h2 id="函数间隔和几何间隔">函数间隔和几何间隔</h2>
<p><strong>定义</strong>：</p>
<blockquote>
<p>(函数间隔)给定训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,关于样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔为 <span class="math display">\[
\hat\gamma_i=y_i(w\cdot x_i+b)
\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[
\hat \gamma= \min_{i=1,\cdots,N}\hat \gamma_i
\]</span></p>
</blockquote>
<p>函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔是不够的；如果成比例改变w和b，超平面没有改变但是间隔却变为原来的两倍；</p>
<p>因此我们可以对分离超平面的法向量加某些约束，如规范化，<span class="math inline">\(||w||=1\)</span>,使得间隔是确定的，这时函数间隔为几何间隔 <span class="math display">\[
\gamma_i=\frac w{||w||}\cdot x_i+\frac b{||w||}
\]</span> <img src="/280b588e/几何间隔.png"></p>
<p>定义</p>
<blockquote>
<p>（几何间隔）对于给定的训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,定义超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔为 <span class="math display">\[
\gamma_i=y_i(\frac w{||w||}\cdot x_i+\frac b{||w||})
\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[
\gamma= \min_{i=1,\cdots,N} \gamma_i
\]</span></p>
</blockquote>
<p>超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔一般是实例点到朝平面的带符号距离，正确分类时就是实例点到超平面的距离；</p>
<p>函数间隔和几何间隔有如下关系 <span class="math display">\[
\gamma_i=\frac{\hat \gamma_i}{||w||}\\
\gamma=\frac{\hat \gamma}{||w||}
\]</span> 如果<span class="math inline">\(||w||=1\)</span>,那么函数间隔和几何间隔相等；</p>
<h2 id="硬间隔最大化">(硬)间隔最大化</h2>
<p>支持向量机的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。间隔最大化的分离超平面是唯一的，这里的间隔最大化指硬间隔最大化;</p>
<p>间隔最大化最直观的表现对训练数据集找到几何间隔最大的超平面就意味着要以充分大的确信度对数据进行分类；</p>
<h3 id="最大间隔分离超平面">最大间隔分离超平面</h3>
<p>求解间隔最大的分离超平面，可以表示为下面的最优化约束问题： <span class="math display">\[
\max_{w,b} \gamma\\
s.t. \quad y_i(\frac {\hat w}{||w||}\cdot x_i + \frac b{||w||})\geq\gamma,\quad i = 1,2,\cdots,N
\]</span> 表示我们希望最大化超平面<span class="math inline">\((w,b)\)</span>关于训练数据的几何间隔<span class="math inline">\(\gamma\)</span>,约束条件表示的超平面<span class="math inline">\((w,b)\)</span>关于每个训练样本点的集合间隔至少<span class="math inline">\(\gamma\)</span>;</p>
<p>参考之前集合间隔和函数间隔的关系式，可以将其改写为 <span class="math display">\[
\max_{w,b}\frac{\hat \gamma}{||w||}\\
s.t. \quad y_i(w\cdot x_i + b)\geq \hat \gamma_i,\quad i=1,2,\cdots,N
\]</span> 由于等比例的改变<span class="math inline">\(w,b,\gamma\)</span>也会成比例的变化因此他是一个等价的问题；可以取<span class="math inline">\(\hat \gamma=1\)</span>,而且注意最大化<span class="math inline">\(\frac1{||w||}\)</span>和最小化<span class="math inline">\(\frac12||w||^2\)</span>是等价的，于是就得到了下面的线性可分支持向量机的最优化问题： <span class="math display">\[
\min_{w,b}\quad \frac12||w||^2\\
s.t.\quad y_i(w\cdot x_i + b)-1\geq0,\quad i=1,2,\cdots,N
\]</span> 这是一个凸二次规划问题；</p>
<blockquote>
<p>凸优化问题是指约束最优化问题 <span class="math display">\[
\min_wf(w)\\
s.t.\quad g_i(w)\leq0,\quad i=1,2,\cdots,k\\
h_i(w)=1,\quad i=1,2,\cdots,l
\]</span> 其中，目标函数<span class="math inline">\(f(w)\)</span>和约束函数<span class="math inline">\(g_i(w)\)</span>都是<span class="math inline">\(R^n\)</span>上的连续可微的凸函数，约束函数<span class="math inline">\(h_i(w)\)</span>是<span class="math inline">\(R^n\)</span>上的仿射函数。（<span class="math inline">\(f(x)\)</span>称为仿射函数，如果它满足<span class="math inline">\(f(x)=a\cdot x + b,a\in R^n,b\in R^n,x \in R^n\)</span>）</p>
</blockquote>
<p>当目标函数<span class="math inline">\(f(w)\)</span>是二次函数且约束函数<span class="math inline">\(g_i(w)\)</span>是仿射函数时，上述最优化问题变成凸二次规划问题；如果求解出了最优化问题的解<span class="math inline">\(w^*,b^*\)</span>,那么就可以得到最大间隔分离超平面<span class="math inline">\(w^*\cdot x+ b^*=0\)</span>及分类决策函数<span class="math inline">\(f(x)=sign(w^*+b^*)\)</span>，即线性可分支持向量机；</p>
<h3 id="线性可分支持向量机-最大间隔法-算法">线性可分支持向量机-最大间隔法-算法</h3>
<p>线性可分支持向量机学习算法—最大间隔法</p>
<ol type="1">
<li>构造并求解约束最优化问题</li>
</ol>
<p><span class="math display">\[
\min_{w,b}\quad \frac12||w||^2\\ 
s.t.\quad y_i(w\cdot x_i+b)-1\geq0,\quad i=1,2,\cdots,N \tag{1}
\]</span></p>
<p>​ 并求得最优解<span class="math inline">\(w^*,b^*\)</span>;</p>
<ol start="2" type="1">
<li>由此得到的分离超平面：</li>
</ol>
<p><span class="math display">\[
w^*\cdot x + b^*=0
\]</span></p>
<p>​ 分类决策函数 <span class="math display">\[
f(x)=sign(w^*\cdot x+b^*)
\]</span> <strong>线性可分训练数据集的最大间隔分离朝平面存在唯一的</strong>；</p>
<h3 id="支持向量和间隔边界">支持向量和间隔边界</h3>
<p>线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）,支持向量是使 <span class="math display">\[
y_i(w\cdot x_i+b)-1=0
\]</span> 对<span class="math inline">\(y_i=+1\)</span>的正例点，支持向量在超平面<span class="math inline">\(H_1:w\cdot x +b =1\)</span>;</p>
<p>对<span class="math inline">\(y_i=-1\)</span>的正例点，支持向量在超平面<span class="math inline">\(H_2:w\cdot x +b =-1\)</span>;</p>
<p>其中<span class="math inline">\(H_1,H_2\)</span>上的点就是支持向量；</p>
<p>注意到<span class="math inline">\(H_1,H_2\)</span>平行，没有实例点落在他们中间，它们之间的距离称为间隔，间隔依赖于分离超平面的法向量<span class="math inline">\(w\)</span>,等于<span class="math inline">\(\frac2{||w||}\)</span>,<span class="math inline">\(H_1,H_2\)</span>为间隔边界；<strong>分离超平面只有支持向量起作用，而其他实例点不会影响分离超平面；由于支持向量在确定分离超平面起到决定性的作用，因此这种分类模型叫做支持向量机，支持向量的个数一般很少，所以支持向量由”很少的重要的“训练样本确定；</strong>（支持向量机的命名）</p>
<p><img src="/280b588e/支持向量.png"></p>
<h3 id="学习的对偶算法">学习的对偶算法</h3>
<p>求解线性可分支持向量机的最优化问题(1)可以作为原始最优化问题，应用拉格朗日对偶性可以通过求解对偶问题来得到原始问题的最优解；对偶问题往往更容易求解，而且自然的引入核函数，以便于推广到非线性问题；</p>
<p>首先构建拉格朗日函数，对于每一个约束条件引入拉格朗日乘子<strong><span class="math inline">\(a_i\geq0,i=1,2,\cdots,N\)</span>（这个将用于后面重新从另一个角度阐述支持向量的关键，注意这个条件）</strong>，则： <span class="math display">\[
L(w,b,\alpha)=\frac12||w||^2-\sum\limits^N_{i=1}a_iy_i(w\cdot x_i+b)+\sum^N_{i=1}\alpha_i
\]</span> 根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题： <span class="math display">\[
\max_\alpha\,\min_{w,b}L(w,b,\alpha)
\]</span></p>
<ol type="1">
<li><p>求<span class="math inline">\(\min\limits_{w,b}L(w,b,\alpha)\)</span>，将拉格朗日函数<span class="math inline">\(L(w,b,\alpha)\)</span>分别对<span class="math inline">\(w,b\)</span>,求偏导数并且令其等于0； <span class="math display">\[
\nabla_wL(w,b,\alpha)=w-\sum^N_{i=1}a_iy_ix_i = 0\\
\nabla_bL(w,b,\alpha)=-\sum^N_{i=1}a_iy_i=0
\]</span> 得 <span class="math display">\[
w=\sum^N_{i=1}\alpha_iy_ix_i\\
\sum^N_{i=1}\alpha_iy_i=0
\]</span> 将结论带入拉格朗日函数,得到 <span class="math display">\[
L(w,b,\alpha)=\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}a_iy_i((\sum^N_{j=1}\alpha_jy_jx_j)\cdot x_i+b)+\sum^N_{i=1}\alpha_i\\
=\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_jx_ix_j-b\sum^N_{i=1}a_iy_i+\sum^N_{i=1}\alpha_i\\
=-\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i
\]</span> 因此求解 <span class="math display">\[
\min_{w,b}L(w,b,\alpha)=-\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i
\]</span></p></li>
<li><p>对<span class="math inline">\(\min_{w,b}L(w,b,\alpha)\)</span>对<span class="math inline">\(\alpha\)</span>的极大，即是对偶问题 <span class="math display">\[
\max_\alpha-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\\
s.t.\quad\sum^N_{i=1}\alpha_iy_i=0,\\
\alpha_i\geq0,\,i=1,2,\cdots,N
\]</span></p></li>
</ol>
<p><strong>原始问题满足拉格朗日对偶性中末尾第二个定理的条件，所以存在<span class="math inline">\(w^*,\alpha^*,\beta^*\)</span>,使得<span class="math inline">\(w^*\)</span>是原始问题的解，<span class="math inline">\(\alpha^*,\beta^*\)</span>是对偶问题的解；这就意味着几乎等价的将原始问题转化为对偶问题；</strong>(巧妙~~)</p>
<hr>
<p>对线性可分训练数据集，假设对偶最优化问题对<span class="math inline">\(\alpha\)</span>的解为<span class="math inline">\(\alpha^*=(a^*_1,a^*_2,\cdots,a^*_N)^T\)</span>,可以有<span class="math inline">\(\alpha^*\)</span>求得原始最优化问题的解<span class="math inline">\(w^*,b^*\)</span>;i</p>
<p><strong>定理：</strong> 设<span class="math inline">\(\alpha^*\)</span>是对偶最优化问题的解，则存在下表<span class="math inline">\(j\)</span>,使得<span class="math inline">\(a^*_j&gt;0\)</span>,且 <span class="math display">\[
w^*=\sum^N_{i=1}\alpha^*_iy_ix_i\\
b^*=y_j-\sum^N_{i=1}\alpha^*_iy_i(x_i\cdot x_j)
\]</span></p>
<p><img src="/280b588e/证明从a求解w,b.png"></p>
<p>​ 分类决策函数可以写成 <span class="math display">\[
f(x)=sign(\sum^N_{i=1}\alpha^*_iy_ix\cdot x_i+b^*)
\]</span> 也就是说分裂决策函数只依赖于输入x和训练样本输入的内积；上式称为线性可分支持向量机的对偶形式；</p>
<p><strong>综上所述，对于给定的线性可分训练数据集，可以先求对偶问题的解<span class="math inline">\(\alpha^*\)</span>在根据定理求解原始问题的<span class="math inline">\(w^*,b^*\)</span>;从而得到分离超平面及分类决策函数；这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机的基本算法；</strong></p>
<h3 id="线性可分支持向量机学习算法">线性可分支持向量机学习算法</h3>
<p><img src="/280b588e/线性可分支持向量机学习算法.png"></p>
<p>线性可分支持向量机中，<span class="math inline">\(w^*,b^*\)</span>只依赖于训练数据中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>,而其他样本点对<span class="math inline">\(w^*,b^*\)</span>没有影响我们将训练数据中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的实例点<span class="math inline">\(x_i\in R^n\)</span>称为支持向量；</p>
<p><strong>定义：（支持向量）</strong> 考虑最优化问题和对偶最优化问题，将训练数据集中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>的实例<span class="math inline">\(x_i\in R^n\)</span>称为支持向量；</p>
<p>根据这一定义支持向量一定在间隔边界上，由KKT互补条件可知， <span class="math display">\[
a^*_i(y_i(w^*\cdot x_i+b^*)-1)=0,\quad i=1,2,\cdots,N
\]</span> 对应于<span class="math inline">\(a^*_i&gt;0\)</span>的实例<span class="math inline">\(x_i\)</span>,有 <span class="math display">\[
y_i(w^*\cdot x_i+b^*)-1=0\\
w^*\cdot x_i+b^*=\pm1
\]</span> 即<span class="math inline">\(x_i\)</span>一定在间隔边界上。这里支持向量的定义与前面给出支持向量是一致的；</p>
<h1 id="线性支持向量机与软间隔最大化">线性支持向量机与软间隔最大化</h1>
<h2 id="线性支持向量机">线性支持向量机</h2>
<blockquote>
<p>线性可分的问题的支持向量学习方法，对线性不可分的训练数据是不适用的，因为这时上述方法的不等式约束并不都能成立。这就需要修改硬间隔最大化为软间隔最大化；</p>
</blockquote>
<p>线性不可分就意味着某些样本点<span class="math inline">\((x_i,y_i)\)</span>不能满足函数间隔大于等于1的约束条件。为了解决这个问题可以对每个样本点<span class="math inline">\((x_i,y_i)\)</span>引入一个松弛变量<span class="math inline">\(\xi_i\geq0\)</span>,使函数间隔加上松弛变量大于等于1，这样约束条件为 <span class="math display">\[
y_i(w\cdot x_i +b)\geq 1-\xi _i
\]</span> 同时，对每个松弛变量<span class="math inline">\(\xi_i\)</span>，支付一个代价<span class="math inline">\(\xi_i\)</span>,目标函数有原来的<span class="math inline">\(\frac12||w||^2\)</span>变成 <span class="math display">\[
\frac12||w||^2+C\sum^N_{i=1}\xi_i
\]</span> 这里，<span class="math inline">\(C&gt;0\)</span>称为惩罚参数，一般根据实际问题决定，<em>C</em>值的大小决定了对误分类的惩罚大小；最小化目标函数包含两层含义：使<span class="math inline">\(\frac12||w||^2\)</span>尽量小（间隔尽量大），同时误分类点个数尽量小，<em>C</em>是用来调和二者的系数；</p>
<p>利用此思路，可以和训练数据线性可分时一样来考虑数据集线性不可分时的线性支持向量机学习问题，称为软间隔最大化；线性不可分的的线性支持向量机学习问题变成了如下凸二次规划问题: <span class="math display">\[
\min_{w,b,\xi}\frac12||w||^2+C\sum^N_{i=1}\xi_i\\
s.t.\quad y_i(w\cdot x_i+b)\geq1-\xi_i,\quad i=1,2,\cdots,N\\
\xi_i\geq,\quad i=1,2,\cdots,N
\]</span> 可以证明<em>w</em>是唯一的，但<em>b</em>的解可能不唯一，而是存在一个区间；</p>
<p><strong>定义（线性支持向量机）：</strong> 对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题，得到分离超平面为 <span class="math display">\[
w^*\cdot x+b^*=0
\]</span> 以及相应的分类决策函数 <span class="math display">\[
f(x)=sign(w^*\cdot x+b^*)
\]</span> 称为线性支持向量机；</p>
<h2 id="学习的对偶算法-1">学习的对偶算法</h2>
<p>原始最优化问题的拉格朗日函数是 <span class="math display">\[
L(w,b,\xi,\alpha,\mu)=\frac12||w||^2+C\sum^N_{i=1}\xi_i-\sum^N_{i=1}\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum^N_{i=1}\mu_i\xi_i
\]</span> 其中，<span class="math inline">\(\alpha_i\geq0,\mu_i\geq0\)</span>,对偶问题是拉格朗日极大极小问题，首先对<span class="math inline">\(L(w,b,\xi,\alpha,\mu)\)</span>对<span class="math inline">\(w,b,\xi\)</span>的极小，由 <span class="math display">\[
\nabla_wL(w,b,\xi,\alpha,\mu)=w-\sum^N_{i=1}\alpha_iy_ix_i=0\\
\nabla_bL(w,b,\xi,\alpha,\mu)=-\sum^N_{i=1}\alpha_iy_i=0\\
\nabla_{\xi_i}L(w,b,\xi,\alpha,\mu)=C-\alpha_i-\mu_i=0
\]</span> 得到 <span class="math display">\[
w=\sum^N_{i-1}\alpha_iy_ix_i\\
\sum^N_{i=1}\alpha_iy_i=0\\
C-\alpha_i-\mu_i=0
\]</span> 将其带入得到 <span class="math display">\[
\min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)=-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i
\]</span> 再对<span class="math inline">\(\min \limits_{w,b,\xi}\,L(w,b,\xi,\alpha,\mu)\)</span>求<span class="math inline">\(\alpha\)</span>的极大，即得对偶问题； <span class="math display">\[
\max_\alpha-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\\
s.t.\quad\sum^N_{i=1}\alpha_iy_i=0\\
C-\alpha_i-\mu_i=0\\
\alpha_i\geq0\\
\mu_i\geq 0 ,\quad i=1,2,\cdots,N
\]</span> 可以将其转化为 <span class="math display">\[
\min_\alpha \frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\alpha_i\\
s.t.\quad \sum^N_{i=1}\alpha_iy_i=0\\
0\leq\alpha_i\leq C,\quad i=1,2,\cdots,N
\]</span> <strong>定理：</strong>设<span class="math inline">\(\alpha^*\)</span>是对偶问题的一个解，若存在<span class="math inline">\(\alpha^*\)</span>的一个分量<span class="math inline">\(\alpha^*,0&lt;\alpha^*_j&lt;C\)</span>,则原始问题的解<span class="math inline">\(w^*,b^*\)</span>,可以按照下式： <span class="math display">\[
w^*=\sum^N_{i=1}\alpha^*_iy_ix_i\\
b^*=y_j-\sum^N_{i=1}y_i\alpha^*_i(x_i\cdot x_j)
\]</span> 证明:</p>
<p><img src="/280b588e/证明支持向量机求w,b.png"></p>
<p><img src="/280b588e/证明支持向量机求w,b下.png"></p>
<p>由此可知，分离超平面 <span class="math display">\[
\sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*=0
\]</span> 分类决策函数可以写成 <span class="math display">\[
f(x)=sign(\sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*)
\]</span></p>
<h2 id="算法">算法</h2>
<p><img src="/280b588e/线性支持向量机学习算法1.png"></p>
<p><img src="/280b588e/线性支持向量机学习算法2.png"></p>
<h2 id="支持向量">支持向量</h2>
<p>在线性不可分的情况下，将对偶问题的解<span class="math inline">\(\alpha^*\)</span>中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>的实例<span class="math inline">\(x_i\)</span>称为支持向量（软间隔的支持向量）。如图</p>
<p><img src="/280b588e/软间隔的支持向量.png"></p>
<p>软间隔的支持向量在间隔边界或者分离超平面之间，</p>
<ul>
<li>若<span class="math inline">\(\alpha^*_i&lt;C\)</span>,则<span class="math inline">\(\xi_i=0\)</span>,支持向量恰好落在间隔边界上</li>
<li>若<span class="math inline">\(\alpha^*_i=C,0&lt;\xi_i&lt;1,\)</span>则分类正确，实例落在间隔边界与分离超平面之间</li>
<li>若<span class="math inline">\(\alpha^*_i=C,\xi_i=1\)</span>,则实例在分离超平面上</li>
<li>若<span class="math inline">\(\alpha^*_i=C,\xi_i&gt;1\)</span>,则实例点位于分离超平面误分的一侧</li>
</ul>
<h2 id="合页损失函数">合页损失函数</h2>
<p>线性可分支持向量机学习还有另外一种解释，就是最小化以下目标函数 <span class="math display">\[
\sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2
\]</span> 目标函数的第一项是经验损失或经验风险，函数 <span class="math display">\[
L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+
\]</span> 称为合页损失函数(hinge loss function)。下标“+”表示一下取正值的函数 <span class="math display">\[
[z]_+=\begin{cases}
z,z&gt;0\\
0,z\leq0
\end{cases}
\]</span> 这就是说，当样本点<span class="math inline">\((x_i,y_i)\)</span>被正确分类且间隔（置信度）<span class="math inline">\(y_i(w\cdot x_i+b)\)</span>大于1时，损失是0，否则损失是<span class="math inline">\(1-y_i(w\cdot x_i+b)\)</span>.注意到其中有正确分类但损失不是0.目标函数的第二项是函数<span class="math inline">\(\lambda\)</span>的w的<span class="math inline">\(L_2\)</span>范数，是正则化项；</p>
<p><strong>定理：</strong> 线性支持向量机的原始最优化问题 <span class="math display">\[
\min_{w,b,\xi}\frac12||w||^2+C\sum^N_{i=1}\xi\\
s.t.\quad y_i(w\cdot x_i +b)\geq1-\xi,\quad i=1,2,\cdots,N\\
\xi_i\geq0,\quad i=1,2,\cdots,N\tag{1,2,3}
\]</span> 等价于最优化问题 <span class="math display">\[
\min_{w,b}\sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2
\]</span> <strong>证明:</strong>可以将上面两个式子进行改写，令 <span class="math display">\[
[1-y_i(w\cdot x_i+b)]_+=\xi_i
\]</span></p>
<ul>
<li>由合页函数定义知<span class="math inline">\(\xi_i\geq0\)</span>成立</li>
<li>当<span class="math inline">\(1-y_i(w\cdot x_i+b)&gt;0\)</span>时，有<span class="math inline">\(1-y_i(w\cdot x_i)=1-\xi_i\)</span></li>
<li>当<span class="math inline">\(q-y_i(w\cdot x_i+b)\leq0\)</span>时，<span class="math inline">\(\xi_i=0\)</span>,有<span class="math inline">\(y_i(w\cdot x_i+b)\geq1-\xi_i\)</span></li>
</ul>
<p>于是，满足约束条件故最优化问题可以写为 <span class="math display">\[
\min_{w,b}\sum^N_{i=1}\xi_i+\lambda||w||^2
\]</span> 若取<span class="math inline">\(\lambda=\frac1{2C}\)</span>,则 <span class="math display">\[
\min_{w,b}\frac1C(\frac12||w||^2+C\sum^N_{i=1}\xi_i)
\]</span></p>
<hr>
<p>依据下图来做一个简单的总结</p>
<p><img src="/280b588e/合页损失函数.png"></p>
<ul>
<li>0-1损失函数可以认为是二分类问题的真正损失函数，而合页损失函数是其上界；由于0-1损失函数不是连续可导的，直接优化目标函数比较困难，可以认为线性支持向量机是由优化0-1损失函数的上界（合页损失函数）构成的目标函数。此上界又称为代理损失函数；</li>
<li>图中虚线显示的是感知机的损失函数<span class="math inline">\([-y_i(w\cdot x_i+b)]_+\)</span>,当样本点被正确分类时，损失为0，否则损失为<span class="math inline">\(-y_i(w\cdot x_i+b)\)</span>相比之下，<strong>合页损失函数不仅要求分类正确，而且确信度足够高时损失才是0，合页损失函数有更高的要求</strong>；</li>
</ul>
<h1 id="非线性支持向量机与核函数">非线性支持向量机与核函数</h1>
<blockquote>
<p>这里介绍说的非线性支持向量机主要特点是利用核技巧（kernel trick），因此会介绍核技巧，其不仅用于支持向量机也用于其他统计学习问题；</p>
</blockquote>
<h2 id="核技巧">核技巧</h2>
<p>一般来说，对给定的一个训练数据集如果一个超曲面能够将正负例正确分开，但可以用一条椭圆曲线（非线性模型）将其分开，则称这个问题为非线性可分问题；</p>
<p>我们所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题求解原来的非线性问题。通过变换将作图中的椭圆变换成右图中的直线，将非线性问题变换为线性可分类问题；（巧妙）</p>
<p><img src="/280b588e/非线性分类问题与核技巧示例.png"></p>
<p>根据上图，用线性分类方法求解线性分类问题可以分成两部</p>
<ul>
<li>使用一个变换将空间中的数据映射到新的空间中</li>
<li>然后再新的空间中使用线性分类学习方法从训练数据中学习分类模型</li>
</ul>
<p>核技巧就是这样的方法，核技巧应用支持向量机基本想法是通过一个非线性变换将输入空间（欧式空间<span class="math inline">\(R^b\)</span>或离散集合）对应于一个特征空间（西伯尔特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机来完成；</p>
<p><strong>核函数的定义</strong>:设输入空间<span class="math inline">\(\chi\)</span>，特征空间<span class="math inline">\(H\)</span>,如果存在一个映射 <span class="math display">\[
\phi(x):\chi\rightarrow H
\]</span> 使得对所有的<span class="math inline">\(x,z\in\chi,\)</span>函数<span class="math inline">\(K(x,z)\)</span>满足条件 <span class="math display">\[
K(x,z)=\phi(x)\cdot\phi(z)
\]</span> 则称<span class="math inline">\(K(x,z)\)</span>为核函数，<span class="math inline">\(\phi(x)\)</span>为映射函数，核函数为映射函数的内积；</p>
<p>核技巧的想法是，在学习与预测中只定义核函数，而不显示的定义映射函数，通常直接定义<span class="math inline">\(K(x,z)\)</span>比较容易，而通过映射函数计算核函数并不容易特征；注意,<strong><span class="math inline">\(\phi\)</span></strong>是输入空间到特征空间的映射，特征空间一般是高维的，甚至是无穷维的，可以看到对给定的核<span class="math inline">\(K(x,z)\)</span>,特征空间和映射函数的取法不唯一，甚至在同意特征空间内也可以取不同的映射；</p>
<h2 id="核技巧在支持向量中的应用">核技巧在支持向量中的应用</h2>
<blockquote>
<p>注意到线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积<span class="math inline">\(x_i\cdot x_j\)</span>,可以用核函数<span class="math inline">\(K(x_i,x_j)\)</span>来代替。</p>
</blockquote>
<p>此时对偶问题的目标函数为 <span class="math display">\[
W(\alpha)=\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum^N_{i=1}\alpha_i
\]</span> 同样分类决策函数中的内积也可以用核函数代替，因为分类决策函数为 <span class="math display">\[
f(x)=sign(\sum^{N_s}_{i=1}\alpha^*_iy_i\phi(x_i)\cdot\phi(x)+b^*)\\
=sign(\sum^{N_S}_{i=1}\alpha^*_iy_iK(x_i,x)+b^*)
\]</span> 这等价于经过映射函数<span class="math inline">\(\phi\)</span>将原来的输入空间转换到一个新的特征空间，将输入空间中的内积<span class="math inline">\(x_i,x_j\)</span>,变换为特征空间中的内积<span class="math inline">\(\phi(x_i)\cdot\phi(y)\)</span>,在新的特征空间中训练样本中学习线性支持向量机，当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型；</p>
<p>在核函数给定的条件下可以利用求解线性分类问题的方法求解非线性分类问题；学习是隐式的在特征空间中进行的，不需要显式定义特征空间和映射函数。这称为核技巧；在实际应用中，往往依赖领域知识直接选择核函数，核函数的选择的有效性需要通过实验验证；</p>
<h2 id="正定核">正定核</h2>
<p>不构造映射<span class="math inline">\(\phi(x)\)</span>能否判断一个给定的函数<span class="math inline">\(K(x,z)\)</span>是不是核函数？函数<span class="math inline">\(K(x,z)\)</span>满足什么条件才能成为核函数？</p>
<p>这里叙述正定核的充要条件，通常所说的核函数就是正定核函数（positive definite kernel function)；</p>
<blockquote>
<p>假设<span class="math inline">\(K(x,z)\)</span>是定义在<span class="math inline">\(\chi * \chi\)</span>上的对称函数，并且对任意的<span class="math inline">\(x_1,x_2,\cdots,x_m\in\chi ,K(x,z)\)</span>关于<span class="math inline">\(x_1,x_2,\cdots,x_m\)</span>的Gram矩阵是半正定的，可以依据函数<span class="math inline">\(K(x,z)\)</span>,构成一个希尔伯特空间，其步骤为</p>
<ul>
<li>先定义映射函数<span class="math inline">\(\phi\)</span>并构成向量空间<em>S</em>;</li>
<li>然后再<em>S</em>空间上定义内积构成内积空间；</li>
<li>最后<em>S</em>完备化构成希尔伯特空间</li>
</ul>
</blockquote>
<ol type="1">
<li><p>定义映射,构成向量空间<em>S</em></p>
<p>先定义映射 <span class="math display">\[
\phi:x\rightarrow K(\cdot,x)
\]</span> 根据这一映射，对任意<span class="math inline">\(x_i\in\chi,\alpha_i\in R,i=1,2,\cdots,m\)</span>,定义线性组合 <span class="math display">\[
f(\cdot)=\sum^M_{i=1}\alpha_iK(\cdot,x_i)
\]</span> 考虑线性组合为元素的集合<span class="math inline">\(S\)</span>,由于集合<span class="math inline">\(S\)</span>对加法和乘法运算是封闭的，所以<em>S</em>构成一个向量空间；</p></li>
<li><p>在<em>S</em>上定义内积，使之称为内积空间</p>
<p>在S上定义运算<span class="math inline">\(*\)</span>；对任意<span class="math inline">\(f,g\in S\)</span>, <span class="math display">\[
f(\cdot)=\sum^M_{i=1}\alpha_iK(\cdot,x_i)\\
g(\cdot)=\sum^l_{j=1}\beta_jK(\cdot,z_i)
\]</span> 定义运算<span class="math inline">\(*\)</span> <span class="math display">\[
f*g=\sum^M_{i=1}\sum^L_{j=1}\alpha_i\beta_jK(x_i,z_j)
\]</span> <img src="/280b588e/证明定义运算是s的内积_1.png"></p>
<p><img src="/280b588e/证明定义运算是s的内积_2.png"></p></li>
<li><p>将内积空间<span class="math inline">\(S\)</span>完备化为希尔伯特空间</p>
<p><img src="/280b588e/内积空间S完备化.png"></p></li>
<li><p><strong>正定核的充要条件</strong> 设<span class="math inline">\(K:\chi * \chi\rightarrow R\)</span>是对称函数，则<span class="math inline">\(K(x,z)\)</span>为正定核函数的充要条件是对任意的<span class="math inline">\(x_i\in \chi,i=1,2,\cdots,m,K(x,z)\)</span>对应的Gram矩阵： <span class="math display">\[
K=[K(x_i,x_j)]_{m*m}
\]</span> 是半正定的；</p>
<p><strong>定义：（正定核的等价定义)</strong>: 设<span class="math inline">\(\chi\subset R^n，K(x,z)\)</span>是定义在<span class="math inline">\(\chi*\chi\)</span>上的对称函数，对任意的<span class="math inline">\(x_i\in \chi,i=1,2,\cdots,m,K(x,z)\)</span>对应的Gram矩阵 <span class="math display">\[
K=[K(x_i,x_j)]_{m*m}
\]</span> 是半正定矩阵，则称<span class="math inline">\(K(x,z)\)</span>是正定核；</p>
<p><strong>此定义在构造核函数时候很有用，但对于一个具体的核函数来说验证它是否为正定核是不容易的，因为要求对任意有限输入集<span class="math inline">\(\{x_1,x_2,\cdots,x_m\}\)</span>验证<em>K</em>对应的Gram矩阵是否为半正定的。实际问题中往往应用已有的核函数。另外，由Mercer定理可以得到Mercer核，正定核比Mercer更具一般性</strong>；</p></li>
</ol>
<h2 id="常用核函数">常用核函数</h2>
<p><img src="/280b588e/常用核函数_1.png"></p>
<p><img src="/280b588e/常用核函数_2.png"></p>
<h2 id="非线性支持向量分类机141">非线性支持向量分类机（141）</h2>
<p>待更新。。。。</p>

    </div>

    
    
    

    <div>
      
        ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>
        <div class="reward-container">
  <div>作者将会持续总结、分享，如果愿意支持作者，作者将会十分感激！！！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Gonlnib 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Gonlnib 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>





      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%94%BB%E5%85%8B-%E5%B0%8F%E8%93%9D%E4%B9%A6/" rel="tag"><i class="fa fa-tag"></i> 攻克'小蓝书'</a>
              <a href="/tags/%E5%8E%9F%E7%90%86/" rel="tag"><i class="fa fa-tag"></i> 原理</a>
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 学习笔记</a>
              <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag"><i class="fa fa-tag"></i> 支持向量机</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/dff65b00/" rel="prev" title="k近邻法">
      <i class="fa fa-chevron-left"></i> k近邻法
    </a></div>
      <div class="post-nav-item">
    <a href="/32703/" rel="next" title="logistic回归&最大熵模型">
      logistic回归&最大熵模型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#引言"><span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性可分支持向量机"><span class="nav-text">线性可分支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#函数间隔和几何间隔"><span class="nav-text">函数间隔和几何间隔</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#硬间隔最大化"><span class="nav-text">(硬)间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#最大间隔分离超平面"><span class="nav-text">最大间隔分离超平面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性可分支持向量机-最大间隔法-算法"><span class="nav-text">线性可分支持向量机-最大间隔法-算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量和间隔边界"><span class="nav-text">支持向量和间隔边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习的对偶算法"><span class="nav-text">学习的对偶算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性可分支持向量机学习算法"><span class="nav-text">线性可分支持向量机学习算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性支持向量机与软间隔最大化"><span class="nav-text">线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性支持向量机"><span class="nav-text">线性支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习的对偶算法-1"><span class="nav-text">学习的对偶算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法"><span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量"><span class="nav-text">支持向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#合页损失函数"><span class="nav-text">合页损失函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#非线性支持向量机与核函数"><span class="nav-text">非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#核技巧"><span class="nav-text">核技巧</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核技巧在支持向量中的应用"><span class="nav-text">核技巧在支持向量中的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正定核"><span class="nav-text">正定核</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用核函数"><span class="nav-text">常用核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非线性支持向量分类机141"><span class="nav-text">非线性支持向量分类机（141）</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gonlnib"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">Gonlnib</p>
  <div class="site-description" itemprop="description">Gonlnib 的个人博客，主要内容会涉及一些学习记录、思考、理解，也会有一些个人爱好的展示</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">gonlnib</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">31k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>









         <div class=BbeiAn-info" style="color:black pos" align="center">
                  陕ICP备 -
                  <a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">20001965号</a>
                  </a>
         </div>

      </div>


    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='1' zIndex='-1' count='199' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@latest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail',];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'RmyI9HlT5Oh3mf6JFv93EYol-gzGzoHsz',
      appKey     : 'IArUETpHH06QrO50jMFkGho5',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
