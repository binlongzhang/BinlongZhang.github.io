<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>拉格朗日对偶性</title>
      <link href="//22545/"/>
      <url>//22545/</url>
      
        <content type="html"><![CDATA[<h1 id="原始问题">原始问题</h1><p>假设<span class="math inline">\(f(x),c_i(x),h_j(x)\)</span>是定义在<span class="math inline">\(R^n\)</span>上连续可微函数，最优化问题 <span class="math display">\[\min_{x\in R^n}\quad f(x)\\s.t \quad c_j\leq0,\quad i=1,2,\cdots,k\\h_j(x)=0,\quad j= 1,2,\cdots,l \tag{1}\]</span> 引入广义拉格朗日函数 <span class="math display">\[L(x,\alpha,\beta)=f(x)+\sum^k_{i=1}\alpha_ic_i(x)+\sum^l_{j=1}\beta_jh_j(x)\]</span> 其中，<span class="math inline">\(\alpha_i,\beta_j\)</span>是拉格朗日乘子，<span class="math inline">\(\alpha_i\geq 0\)</span>,考虑<span class="math inline">\(x\)</span>的函数： <span class="math display">\[\theta_p(x)\quad=\max_{\alpha,\beta:\alpha_i\geq0}\quad L(x,\alpha,\beta)\]</span> 这里，下表P表示原始问题；假设给定某个x，如果违反原始问题的约束条件，就有 <span class="math display">\[\theta_p(x)\quad=\max_{\alpha,\beta:\alpha_i\geq0}\quad L(x,\alpha,\beta)=+\infty\]</span> 因为若某个i使约束<span class="math inline">\(c_i(x)&gt;0\)</span>,则可令<span class="math inline">\(\alpha_i\rightarrow+\infty\)</span>,若某个j使<span class="math inline">\(h_j(x)\neq 0\)</span>则可令<span class="math inline">\(\beta_i\rightarrow+\infty\)</span>,而将其余的<span class="math inline">\(\alpha_i,\beta_i\)</span>均设为0；因此 <span class="math display">\[\theta_p(x)=\begin{cases} f(x),x满足原始问题约束\\+\infty,其他\\\end{cases}\]</span> 因此考虑极小化问题 <span class="math display">\[\min_x\theta_p(x)=\min_x\,\max_{\alpha,\beta:\alpha_i\geq0}\,L(x,\alpha,\beta)\]</span> 他和（1）是等价的；问题<span class="math inline">\(\min_x\,\max_{\alpha,\beta:\alpha_i\geq0}\,L(x,\alpha,\beta)\)</span>称为广义拉格朗日函数的极小极大问题，这样就把原始最优化问题表示为广义拉格朗日函数的极小极大值问题。为了方便定义原始问题的最优解 <span class="math display">\[p^*=\min_x\theta_p(x)\]</span> 称为原始问题的值；</p><h1 id="对偶问题">对偶问题</h1><p>定义 <span class="math display">\[\theta_D(\alpha,\beta) = \min_xL(x,\alpha,\beta)\]</span> 再考虑极大化<span class="math inline">\(\theta_D(\alpha,\beta)\)</span>,即 <span class="math display">\[\max_{\alpha,\beta:\alpha\geq0}\theta_D(\alpha,\beta)= \max_{\alpha,\beta:\alpha\geq0}\min_xL(x,\alpha,\beta)\]</span> 称为广义拉格朗日函数的极大极小问题；</p><p>于是广义拉格朗日函数的极大极小问题表示为约束优化问题： <span class="math display">\[\max_{\alpha,\beta:\alpha\geq0}\theta_D(\alpha,\beta)= \max_{\alpha,\beta:\alpha\geq0}\min_xL(x,\alpha,\beta)\\s.t. \alpha_i\geq0,i=1,2,\cdots,k\]</span> 称为原始问题的对偶问题，定义对偶问题最优值 <span class="math display">\[d^*=\max_{\alpha,\beta:\alpha\geq0}\theta_D(\alpha,\beta)\]</span> 称为对偶问题的值；</p><h1 id="原始问题和对偶问题的关系">原始问题和对偶问题的关系</h1><ul><li><p>若原始问题和对偶问题都有最优解，则<span class="math inline">\(d^*\leq p^*\)</span>；</p><ul><li>推论：设<span class="math inline">\(x^*，\alpha^*,\beta^*\)</span>分别为原始问题和对偶问题的可行解，并且<span class="math inline">\(d^*=p^*\)</span>,则<span class="math inline">\(x^*，\alpha^*,\beta^*\)</span>分别是原始问题和对偶问题的最优解；</li></ul></li><li><p>假设<span class="math inline">\(f(x),c_i(x)\)</span>是凸函数，<span class="math inline">\(h_j(x)\)</span>是仿射函数；并且假设不等式约束<span class="math inline">\(c_i(x)\)</span>是严格可行的(存在x，对所有的i有<span class="math inline">\(c_i{(x)}\leq0\)</span>),则存在<span class="math inline">\(x^*，\alpha^*,\beta^*\)</span>,使<span class="math inline">\(x^*\)</span>是原始问题的解，<span class="math inline">\(\alpha^*,\beta^*\)</span>是对偶问题的解，且 <span class="math display">\[p^*=d^*=L(x^*，\alpha^*,\beta^*)\]</span></p></li><li><p>假设<span class="math inline">\(f(x),c_i(x)\)</span>是凸函数，<span class="math inline">\(h_j(x)\)</span>是仿射函数；并且假设不等式约束<span class="math inline">\(c_i(x)\)</span>是严格可行的,则存在<span class="math inline">\(x^*，\alpha^*,\beta^*\)</span>分别是原始问题和对偶问题的解的充分必要条件是<span class="math inline">\(x^*，\alpha^*,\beta^*\)</span>满足下面的（KKT）条件：</p></li></ul><p><img src="/22545/KKT条件.png"></p><p>​ 特别指出，式（c.22）称为KKT的对偶互补条件，由此条件可知：若<span class="math inline">\(a_i^*&gt;0\)</span>,则<span class="math inline">\(c_i(x^*)=0\)</span>;</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 拉格朗日对偶性 </tag>
            
            <tag> 数学基础 </tag>
            
            <tag> 最优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>logistic回归&amp;最大熵模型</title>
      <link href="//32703/"/>
      <url>//32703/</url>
      
        <content type="html"><![CDATA[<h1 id="logistic回归模型">logistic回归模型</h1><h2 id="logistic分布">logistic分布</h2><p><strong>定义logistic分布</strong> <span class="math display">\[F(x)=P(X\leq x)=\frac1{1+e^{-(x-\mu)/\gamma}}\\f(x)=F^{&#39;}(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\mu})^2}\]</span> 分布函数图像是一条S形曲线，以点<span class="math inline">\((\mu,\frac12)\)</span>为中心对称，如下图</p><p><img src="/32703/logistic分布.png"></p><h2 id="二项logistic回归模型">二项logistic回归模型</h2><p><strong>定义二项logistic是如下条件概率分布</strong> <span class="math display">\[p(Y=1|x)=\frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)}\\p(Y=0|x)=\frac1{1+\exp(w\cdot x+b)}\]</span> 为了方便有时将偏置并入权重中，即<span class="math inline">\(w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^T,x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)\)</span>,同时<strong><em>logistic</em></strong>模型简写为 <span class="math display">\[p(Y=1|x)=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\p(Y=0|x)=\frac1{1+\exp(w\cdot x)}\]</span> 定义对数几率（log odds)或<strong><em>logit</em></strong>函数为 <span class="math display">\[logit(p)=log\frac p{1-p}\]</span> 对logistc回归而言 <span class="math display">\[log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x\]</span> 说明了在<em>logistic</em>回归中，输出<span class="math inline">\(Y=1\)</span>的对数概率是输入<span class="math inline">\(x\)</span>的线性函数；通过<em>logistic</em>回归定义模型可以将线性函数<span class="math inline">\(w\cdot x\)</span>转换为概率；</p><h2 id="模型的参数估计">模型的参数估计</h2><p>对于给定训练数据集可以应用极大似然估计法估计模型参数，</p><p>设 <span class="math display">\[P(Y=1|x)=\pi(x),\,\,\,P(Y=1|x)=1-\pi(x)\]</span> 似然函数为 <span class="math display">\[\prod^N_{i=1}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\]</span> 对数似然函数 <span class="math display">\[L(w)=\sum\limits ^N_{i=1}[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\=\sum\limits^N_{i=1}[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)]\]</span> 对<span class="math inline">\(L(w)\)</span>求极大值，得到<span class="math inline">\(w\)</span>的估计值；至此通常采用梯度下降法或者拟牛顿法来求解；</p><h2 id="多项logistic回归">多项logistic回归</h2><p>将其从二项推广到多项<em>logistic</em>回归模型 <span class="math display">\[P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum\limits^{k-1}_{k=1}\exp(w_k\cdot x)},k=1,2,\cdots,K-1\\P(Y=K|x)=\frac1{1+\sum\limits^{k-1}_{k=1}\exp(w_k\cdot x)}\]</span> 同理，二项<em>logistic</em>回归的参数估计法也可以推广到多项<em>logistic</em>回归；</p><h1 id="最大熵模型">最大熵模型</h1><h2 id="原理">原理</h2><p>最大熵模型是概率模型学习的一个准则，最大熵原理认为，在所有可能的概率模型中，熵最大的是最好的模型，通常用约束条件确定概率模型的集合；设离散变量<span class="math inline">\(X\)</span>分布是<span class="math inline">\(P(X)\)</span>，熵为 <span class="math display">\[H(P)=-\sum\limits_xP(x)\log P(x)\]</span> 且满足不等式 <span class="math display">\[0\leq H(P) \leq \log|X|\]</span> 直观的，在没有更多信息的情况下认为不确定部分是等可能的，“等可能”不易操作，而熵则是一个可优化的指标以此来达到等可能的目的；</p><p><img src="/32703/最大熵原理的概率模型解释.png"></p><h2 id="最大熵模型定义">最大熵模型定义</h2><p>对于给定训练数据集 <span class="math display">\[\hat P(X=x,Y=y)=\frac{v(X=x,Y=y)}N\\\hat P(X=x)=\frac{v(X=x)}N\\其中v(X=x,Y=y)表示样本(x,y)出现的频数；\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>定义 <span class="math display">\[f(x)=\left\{ \begin{aligned} 1,&amp;\,\,x与y满足某一事实\\0,&amp;\,否则\end{aligned}\right.\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>关于经验分布<span class="math inline">\(\hat P(X,Y)\)</span>的期望值表示为 <span class="math display">\[E_\hat p(f)=\sum_{x,y}\hat P(x,y)f(x,y)\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>关于模型<span class="math inline">\(P(Y|X)\)</span>与经验分布<span class="math inline">\(\hat P(X)\)</span>的期望值 <span class="math display">\[E_ p(f)=\sum_{x,y}\hat P(x)P(y|x)f(x,y)\]</span> 如果模型能够获取训练数据中的信息，那么可以假设这两个期望值相等 <span class="math display">\[E_p(f)=E_\hat p(f)\]</span> <strong>假设满足所有约束条件的模型集合为</strong> <span class="math display">\[C\equiv\{P\in p|E_P(f_i)=E_\hat p(f_i),i=1,2,\cdots,n\}\]</span> 定义在条件概率分布<span class="math inline">\(P(Y|X)\)</span>上的条件熵为 <span class="math display">\[H(P)=-\sum\limits_{x,y}\hat p(x)P(y|x)\log P(y|x)\]</span> 则模型集合<span class="math inline">\(C\)</span>中条件熵<span class="math inline">\(H(P)\)</span>最大的模型为最大熵模型，其中的对数为自然对数；</p><h2 id="最大熵模型的学习">最大熵模型的学习</h2><p>最大熵模型的学习可以形式化为约束最优化问题；</p><p>对于给定的训练数据集<span class="math inline">\(T\)</span>以及特征函数<span class="math inline">\(f_i(x,y),i=1,2,\cdots,n\)</span>,最大熵模型的学习等价于约束最优化问题 <span class="math display">\[\max\limits_{p\in C}\,H(P)=-\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)\\s.t.\,\,\,\,\,E_p(f_i)=E_{\hat p}(f_i),\,i=1,2,\cdots,n\\\sum_yP(y|x)=1\]</span> 按照习惯可以将其等价的改为最小值问题 <span class="math display">\[\max\limits_{p\in C}\,H(P)=-\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)\\s.t.\,\,\,\,\,E_p(f_i)=E_{\hat p}(f_i),\,i=1,2,\cdots,n\\\sum_yP(y|x)=1\]</span> 求约束最优化问题就是求解最大熵模型。可以将约束最优化问题转化为无约束最优化的对偶问题。通过求解对偶问题来求解原始问题；</p><hr><p><strong>具体推导</strong></p><p>引入拉格朗日乘子<span class="math inline">\(w_0,w_1,\cdots,w_n\)</span>,定义拉格朗日函数<span class="math inline">\(L(P,w)\)</span>; <span class="math display">\[L(P,w)\equiv-H(P)+w_0(1-\sum\limits_yP(y|x))+\sum\limits^n_{i=1}w_i(E_\hat p(f_i)-E_p(f_i))\\=\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x))+\sum^n_{i=1}w_i(\sum\limits_{x,y}\hat P(x,y)f_i(x,y)-\sum_{x,y}\hat P(x)P(y|x)f_i(x,y))\]</span> 最优化原始问题 <span class="math display">\[\min_{p\in C} \max_wL(P,w)\]</span> 对偶问题 <span class="math display">\[\max_w\min_{P\in C}L(P,w)\]</span> 由于拉格朗日函数<span class="math inline">\(L(P,w)\)</span>是<span class="math inline">\(P\)</span>的凸函数，原始问题与对偶问题是等价的（对偶问题的定理)首先记作 <span class="math display">\[\psi(w)=\min_{p\in C}L(P,w)=L(P_w,w)\]</span> <span class="math inline">\(\psi(w)\)</span>称为对偶函数，同时，其解记作 <span class="math display">\[P_w=\arg\min_{p\in C}L(P,w)=P_w(y|x)\]</span> 具体的求<span class="math inline">\(L(P,w)\)</span>对<span class="math inline">\(P(y|x)\)</span>的偏导数，令偏导数等于0，解得 <span class="math display">\[P(y|x)=\exp(\sum_{i=1}^nw_if_i(x,y)+w_0-1)=\frac{\exp(\sum\limits_{i=1}^nw_if_i(x,y))}{\exp(1-w_0)}\]</span> 由于<span class="math inline">\(\sum\limits_yP(y|x)=1\)</span>,得 <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum\limits^n_{i=1}w_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span> 之后求解外部的极大化问题 <span class="math display">\[\max\limits_w\psi(x)\]</span> 记其解为<span class="math inline">\(w^*\)</span>, <span class="math display">\[w^*=\arg\max\limits_w\psi(w)\]</span></p><h2 id="极大似然估计">极大似然估计</h2><p>对偶函数的极大化等价于最大熵模的极大似然估计；</p><p>证明过程：</p><p><img src="/32703/对偶函数极大化等价于最大熵模型极大似然估计.png"></p><p>这样最大熵模型的学习问题就可以转换为具体的求解对数似然函数极大化或对偶函数极大化的问题；写成更一般的形式 <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span></p><h2 id="深入理解">深入理解</h2><p>最大熵模型与<em>logistic</em>回归模型有着类似的形式，它们又称为对数线性模型（log linear model）,模型学习就是在给定的训练数据集条件下对模型进行极大似然估计或正则化的极大似然估计；</p><hr><p>事实上，定义特征函数，其中<span class="math inline">\(g(x)\)</span>为提取出每个x的特征，，输出是<span class="math inline">\(x\)</span>的特征向量： <span class="math display">\[\left\{\begin{aligned} &amp;g(x),y=1\\&amp;0,\quad y=0\end{aligned}\right.\]</span> 将以上特征带入到最大熵模型中 <span class="math display">\[P(y=1|x)=\frac{\exp(w_ig(x))}{\exp(w_ig(x))+\exp(w_i*0)}\]</span> 上下同时除<span class="math inline">\(\exp(w_ig(x))\)</span>,得 <span class="math display">\[P(y=1|x)=\frac1{1+\exp(-w_ig(x))}\]</span> 同理 <span class="math display">\[P(y=0|x)=\frac{\exp(w_i\cdot0)}{\exp(w_ig(x))+\exp(w_i*0)}\\=\frac1{\exp(w_ig(x))+1}\]</span> 自然的发现<em>logistic</em>回归模型其实就是最大熵模型在<span class="math inline">\(y=1\)</span>时抽取x的特征这一情况；之前我们用极大似然估计求参数<span class="math inline">\(w_i\)</span>其实这样求出的模型就是<span class="math inline">\(\max P_w(y|x)\)</span>,所以就是求最大熵模型；</p><p>日常生活中，我们经常不知不觉的就是用了最大熵模型，这里给出了更高层面的抽象的最大熵模型；显然最后的例子也说明了<span class="math inline">\(logistic\)</span>回归其实也是一种最大熵模型；</p><h1 id="模型的最优化算法">模型的最优化算法</h1><p>逻辑斯蒂回归模型、最大熵模型归结为似然函数为莫表的最优化问题，通常通过迭代算法求解，从最优化的角度上来看这时的目标函数具有很好的性质，他是光滑的凸函数；因此多种最优化方法都适用；常用的方法有改进的迭代尺度法、梯度下降法、牛顿法、拟牛顿法。牛顿法或拟牛顿法；牛顿法或者拟牛顿法一般收敛速度更快；</p><p><strong>最大熵模型</strong> <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span> 对数似然函数 <span class="math display">\[L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}w_if_i(x,y)-\sum_x\hat P(x)\log Z_w(x)\]</span></p><h2 id="改进的迭代尺度算法iis">改进的迭代尺度算法(IIS)</h2><h3 id="原理-1">原理</h3><p><strong>IIS</strong>核心想法是：建设最大熵模型当前的参数向量是<span class="math inline">\(w=(w_1,w_2,\cdots,w_n)^T\)</span>,我们希望找到一个新的参数向量<span class="math inline">\(w+\delta=(w_1+\delta_1,w_2+\delta_2,\cdots,w_n+\delta_n)\)</span>,使得模型的对数似然函数值增大。如果能有一种参数更新方法让<span class="math inline">\(w\rightarrow+\delta\)</span>,那么重复使用即可找到对数似然函数的最大值；</p><p>对于给定的经验分布<span class="math inline">\(\hat P(x,y)\)</span>,对数似然函数的该变量是 <span class="math display">\[L(w+\delta)-L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)-\sum_x\hat P(x)\log\frac{Z_{w+\delta}(x)}{Z_w(x)}\]</span></p><p>利用不等式 <span class="math display">\[-\log\alpha\geq1-\alpha,\alpha&gt;0\]</span> 则 <span class="math display">\[-\sum\limits_x\hat P(x)\log\frac{Z_{w+\delta}(x)}{Z_w(x)}\\\geq\sum_x\hat P(x)(1-\frac{Z_{w+\delta}(x)}{Z_w(x)})\\\geq\sum_x\hat P(x)-\sum_x\hat P(x)\frac{Z_{w+\delta}(x)}{Z_w(x)}\\\geq1-\sum_x\hat P(x)\frac{Z_{w+\delta}(x)}{Z_w(x)}\]</span></p><p><span class="math display">\[L(w+\delta)-L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\exp(\sum^n_{i=1}\delta_if_i(x,y))\]</span></p><p>右端记为<span class="math inline">\(A(\delta|w)\)</span>，于是 <span class="math display">\[L(w+\delta)-L(w)\geq A(\delta|w)\]</span> 如果能找到合适的<span class="math inline">\(\delta\)</span>使得下界<span class="math inline">\(A(\delta|w)\)</span>提高，那么对数似然函数也会提高；然而，函数其中的遍量<span class="math inline">\(\delta\)</span>是一个向量含有多个变量，不易同时优化。IIS试图一次只优化其中一个变量<span class="math inline">\(\delta_i\)</span>,而固定其他变量；</p><p>为此引入一个新的量 <span class="math display">\[f^\#(x,y)=\sum\limits_if_i(x,y)\]</span> 于是 <span class="math display">\[A(\delta|w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\exp(f^\#(x,y)\sum^n_{i=1}\frac{\delta_if_i(x,y))}{f^\#(x,y)}\]</span> 由于<span class="math inline">\(\frac{f_i(x,y)}{f^\#(x,y)}\geq0\)</span>且<span class="math inline">\(\sum\limits^n_{i=1}\frac{f_i(x,y)}{f^\#(x,y)}=1\)</span>,根据<span class="math inline">\(Jensen\)</span>不等式，得到 <span class="math display">\[\exp(\sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\delta_if^\#(x,y))\leq \sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\exp(\delta_if^\#(x,y))\]</span> 记<span class="math inline">\(A(\delta|x)\)</span>改写后的为<span class="math inline">\(B(\delta|x)\)</span> <span class="math display">\[B(\delta|x)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\exp(\delta_if^\#(x,y))\]</span> 于是 <span class="math display">\[L(w+\delta)-L(w)\geq B(\delta|w)\]</span> 显然其是对数似然函数的一个新的下界，求<span class="math inline">\(B(\delta|w)\)</span>对<span class="math inline">\(\delta_i\)</span>的偏导数，并令其为0得到 <span class="math display">\[\sum_{x,y}\hat P(x)P_w(y|x)f_i(x,y)\exp(\delta_i,f^\#(x,y))=E_\hat P(f_i)\]</span> 依次对其求解可算出<span class="math inline">\(\delta\)</span>;</p><h3 id="算法">算法</h3><p><em>input:</em>特征函数<span class="math inline">\(f_1,f_2,\cdots,f_n\)</span>,经验分布<span class="math inline">\(\hat P(X,Y)\)</span>,模型<span class="math inline">\(P_w(y|x)\)</span></p><p><em>output:</em>最优参数值<span class="math inline">\(w_i^*\)</span>;最优模型<span class="math inline">\(P_w\)</span></p><ol type="1"><li><p>对所有的<span class="math inline">\(i\in \{1,2,\cdots,n\}\)</span>,取初值<span class="math inline">\(w_i=0\)</span></p></li><li><p>对每一<span class="math inline">\(i\in \{1,2,\cdots,n\}\)</span></p><ol type="1"><li>令<span class="math inline">\(\delta_i\)</span>是方程</li></ol><p><span class="math display">\[\sum_{x,y}\hat P(x)P_w(y|x)f_i(x,y)\exp(\delta_i,f^\#(x,y))=E_\hat P(f_i)\\其中，f^\#(x,y)=\sum\limits_if_i(x,y)\]</span></p><p>的解；</p><ol start="2" type="1"><li>更新<span class="math inline">\(w_i\)</span>值：<span class="math inline">\(w_i\leftarrow w_i+\delta_i\)</span></li></ol></li><li><p>若不是所有的<span class="math inline">\(w_i\)</span>都收敛，重复2</p></li></ol><hr><p>这一算法的关键一步就是2.1,求解其中的<span class="math inline">\(\delta_i\)</span>,如果<span class="math inline">\(f^\#(x,y)\)</span>是常数，则可以显示的表示为 <span class="math display">\[\delta_i=\frac1M\log \frac{E_\hat p(f_i)}{E_p(f_i)}\]</span> 若<span class="math inline">\(f^\#(x,y)\)</span>不是常数，那么必须通过数值计算<span class="math inline">\(\delta_i\)</span>，简单有效的方法就是拟牛顿法；</p><p>以<span class="math inline">\(g(\delta_i)=0\)</span>表示2.1中的方程，牛顿法通过迭代求得的<span class="math inline">\(\delta^*_i\)</span>,使得<span class="math inline">\(g(\delta_i^*)=0\)</span>,迭代公式 <span class="math display">\[\delta_i^{(k+1)}=\delta_i^{(k)}-\frac{g(\delta_i^{(k)})}{g^{&#39;}(\delta_i^{(k)})}\]</span> 只要适当的选取初始值<span class="math inline">\(\delta_i^{(0)}\)</span>,由于<span class="math inline">\(\delta_i\)</span>的方程有单根，因此牛顿法恒收敛，而且收敛速度很快；</p><h3 id="拟牛顿法">拟牛顿法</h3><p>对于最大熵模型而言</p><hr><p><span class="math display">\[P_w(y|x)=\frac{\exp(\sum\limits_{i=1}^nw_if_i(x,y))}{\sum\limits_y\exp(\sum\limits_{i=1}^nw_if_i(x,y))}\\\]</span></p><p>目标函数(极大化似然函数就等价于) <span class="math display">\[\min\limits_{w\in R^n}\quad f(w)=\sum_x\hat P(x)\log\sum_y\exp(\sum^n_{i=1}w_if_i(x,y))-\sum_{x,y}\hat P(x,y)\sum^n_{i=1}w_if_i(x,y)\]</span> 梯度 <span class="math display">\[g(w)=(\frac{\partial f(w)}{\partial w_1},\frac{\partial f(w)}{\partial w_2},\cdots,\frac{\partial f(w)}{\partial w_n})^T\]</span> 其中 <span class="math display">\[\frac{\partial f(w)}{\partial w_i}=\sum\limits_{x,y}\hat P(x)P_w(y|x)f_i(x,y)-E_{\hat P}(f_i),\quad i=1,2,\cdots,n\]</span></p><hr><p>最大熵模型学习的BFGS算法</p><p><em>input</em>:特征函数<span class="math inline">\(f_1,f_2,\cdots,f_n\)</span>;经验分布<span class="math inline">\(\hat P(x,y)\)</span>,目标函数<span class="math inline">\(f(w)\)</span>,梯度<span class="math inline">\(g(w)=\Delta f(w)\)</span>,精度要求<span class="math inline">\(\epsilon\)</span></p><p><em>output</em>:最优参数值<span class="math inline">\(w^*\)</span>；最优模型<span class="math inline">\(P_w\cdot(y|x)\)</span></p><ol type="1"><li>选定初始点<span class="math inline">\(w^{(0)}\)</span>，取<span class="math inline">\(B_0\)</span>为正定对称矩阵，置<span class="math inline">\(k=0\)</span>;</li><li>计算<span class="math inline">\(g_k=g(w^{(k)})\)</span>,若<span class="math inline">\(||g_k||&lt;\epsilon\)</span>,则停止计算，得<span class="math inline">\(w^*=w^{(k)}\)</span>,否则跳转3；</li><li>由<span class="math inline">\(B_kp_k=-g_k\)</span>求出<span class="math inline">\(p_k\)</span>;</li><li>一维搜索：求<span class="math inline">\(\lambda_k\)</span>使得</li></ol><p><span class="math display">\[f(w^{k}+\lambda_kp_k)=\min_{\lambda\geq0}f(w^{(k)}+\lambda p_k)\]</span></p><ol start="5" type="1"><li>置<span class="math inline">\(w^{(k+1)}=w^{(k)}+\lambda_kp_k\)</span>;</li><li>计算<span class="math inline">\(g_{k+1} = g(w^{(k+1)})\)</span>,若<span class="math inline">\(||g_k||&lt;\epsilon\)</span>,则停止计算，得<span class="math inline">\(w^*=w^{(k)}\)</span>,否则求出<span class="math inline">\(B_{k+1}\)</span></li></ol><p><span class="math display">\[B_{k+1}=B_k+\frac{y_ky_k^T}{y^T_k\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta^T_kB_k\delta_k}\\其中，y_k=g_{k+1}-g_k,\delta_k=w^{(k+1)}-w^{(k)}\]</span></p><ol start="7" type="1"><li>置<span class="math inline">\(k=k+1\)</span>,转至3；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 最大熵模型 </tag>
            
            <tag> logistic回归 </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="//280b588e/"/>
      <url>//280b588e/</url>
      
        <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>支持向量机（support vector machines,SVM）是一种二分类模型；它的基本模型定义在特征空间上的间隔最大模型分类器，间隔最大使之区别于感知机；同时还可以使用核技巧，使它成为非线性支持向量机；SVM的学习策略就是间隔最大化，等价于正则化的合页函数最小化；支持向量机的学习算法就是求解凸二次规划的最优化算法；</p><p>由简至繁的模型分别为</p><ul><li>线性可分支持向量机--硬间隔最大化</li><li>线性支持向量机--软间隔最大化</li><li>非线性支持向量机--核技巧+软间隔最大化</li></ul><h1 id="线性可分支持向量机">线性可分支持向量机</h1><p>线性可分支持向量机、线性支持向量机假设输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；支持向量机的学习实在特征空间内进行的；</p><p>假设训练数据集是线性可分的，且对于样本点<span class="math inline">\((x_i,y_i)\)</span>,当<span class="math inline">\(y_i=+1\)</span>时<span class="math inline">\(x_i\)</span>为正例；当<span class="math inline">\(y_i=-1\)</span>时<span class="math inline">\(x_i\)</span>为负例；</p><p>学习的目标是找到分离超平面<span class="math inline">\(w\cdot x+b=0,\quad w\)</span>是法向量，<span class="math inline">\(b\)</span>为截距，法向量指向的是正类；一般的，训练数据集线性可分时存在无穷多个分离超平面将两类数据正确划分。感知机利用误分类最小策略，求得分离超平面；线性可分支持向量机利用间隔最大化求最优分离超平面；</p><p><strong>定义</strong>：（线性可分支持向量机）给定线性可分数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为 <span class="math display">\[w^*\cdot x+b^*=0\]</span> 相应的决策函数为 <span class="math display">\[f(x)=sign(w^*\cdot x+b^*)\]</span></p><h2 id="函数间隔和几何间隔">函数间隔和几何间隔</h2><p><strong>定义</strong>：</p><blockquote><p>(函数间隔)给定训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,关于样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔为 <span class="math display">\[\hat\gamma_i=y_i(w\cdot x_i+b)\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[\hat \gamma= \min_{i=1,\cdots,N}\hat \gamma_i\]</span></p></blockquote><p>函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔是不够的；如果成比例改变w和b，超平面没有改变但是间隔却变为原来的两倍；</p><p>因此我们可以对分离超平面的法向量加某些约束，如规范化，<span class="math inline">\(||w||=1\)</span>,使得间隔是确定的，这时函数间隔为几何间隔 <span class="math display">\[\gamma_i=\frac w{||w||}\cdot x_i+\frac b{||w||}\]</span> <img src="/280b588e/几何间隔.png"></p><p>定义</p><blockquote><p>（几何间隔）对于给定的训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,定义超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔为 <span class="math display">\[\gamma_i=y_i(\frac w{||w||}\cdot x_i+\frac b{||w||})\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[\gamma= \min_{i=1,\cdots,N} \gamma_i\]</span></p></blockquote><p>超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔一般是实例点到朝平面的带符号距离，正确分类时就是实例点到超平面的距离；</p><p>函数间隔和几何间隔有如下关系 <span class="math display">\[\gamma_i=\frac{\hat \gamma_i}{||w||}\\\gamma=\frac{\hat \gamma}{||w||}\]</span> 如果<span class="math inline">\(||w||=1\)</span>,那么函数间隔和几何间隔相等；</p><h2 id="硬间隔最大化">(硬)间隔最大化</h2><p>支持向量机的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。间隔最大化的分离超平面是唯一的，这里的间隔最大化指硬间隔最大化;</p><p>间隔最大化最直观的表现对训练数据集找到几何间隔最大的超平面就意味着要以充分大的确信度对数据进行分类；</p><h3 id="最大间隔分离超平面">最大间隔分离超平面</h3><p>求解间隔最大的分离超平面，可以表示为下面的最优化约束问题： <span class="math display">\[\max_{w,b} \gamma\\s.t. \quad y_i(\frac {\hat w}{||w||}\cdot x_i + \frac b{||w||})\geq\gamma,\quad i = 1,2,\cdots,N\]</span> 表示我们希望最大化超平面<span class="math inline">\((w,b)\)</span>关于训练数据的几何间隔<span class="math inline">\(\gamma\)</span>,约束条件表示的超平面<span class="math inline">\((w,b)\)</span>关于每个训练样本点的集合间隔至少<span class="math inline">\(\gamma\)</span>;</p><p>参考之前集合间隔和函数间隔的关系式，可以将其改写为 <span class="math display">\[\max_{w,b}\frac{\hat \gamma}{||w||}\\s.t. \quad y_i(w\cdot x_i + b)\geq \hat \gamma_i,\quad i=1,2,\cdots,N\]</span> 由于等比例的改变<span class="math inline">\(w,b,\gamma\)</span>也会成比例的变化因此他是一个等价的问题；可以取<span class="math inline">\(\hat \gamma=1\)</span>,而且注意最大化<span class="math inline">\(\frac1{||w||}\)</span>和最小化<span class="math inline">\(\frac12||w||^2\)</span>是等价的，于是就得到了下面的线性可分支持向量机的最优化问题： <span class="math display">\[\min_{w,b}\quad \frac12||w||^2\\s.t.\quad y_i(w\cdot x_i + b)-1\geq0,\quad i=1,2,\cdots,N\]</span> 这是一个凸二次规划问题；</p><blockquote><p>凸优化问题是指约束最优化问题 <span class="math display">\[\min_wf(w)\\s.t.\quad g_i(w)\leq0,\quad i=1,2,\cdots,k\\h_i(w)=1,\quad i=1,2,\cdots,l\]</span> 其中，目标函数<span class="math inline">\(f(w)\)</span>和约束函数<span class="math inline">\(g_i(w)\)</span>都是<span class="math inline">\(R^n\)</span>上的连续可微的凸函数，约束函数<span class="math inline">\(h_i(w)\)</span>是<span class="math inline">\(R^n\)</span>上的仿射函数。（<span class="math inline">\(f(x)\)</span>称为仿射函数，如果它满足<span class="math inline">\(f(x)=a\cdot x + b,a\in R^n,b\in R^n,x \in R^n\)</span>）</p></blockquote><p>当目标函数<span class="math inline">\(f(w)\)</span>是二次函数且约束函数<span class="math inline">\(g_i(w)\)</span>是仿射函数时，上述最优化问题变成凸二次规划问题；如果求解出了最优化问题的解<span class="math inline">\(w^*,b^*\)</span>,那么就可以得到最大间隔分离超平面<span class="math inline">\(w^*\cdot x+ b^*=0\)</span>及分类决策函数<span class="math inline">\(f(x)=sign(w^*+b^*)\)</span>，即线性可分支持向量机；</p><h3 id="线性可分支持向量机-最大间隔法-算法">线性可分支持向量机-最大间隔法-算法</h3><p>线性可分支持向量机学习算法—最大间隔法</p><ol type="1"><li>构造并求解约束最优化问题</li></ol><p><span class="math display">\[\min_{w,b}\quad \frac12||w||^2\\ s.t.\quad y_i(w\cdot x_i+b)-1\geq0,\quad i=1,2,\cdots,N \tag{1}\]</span></p><p>​ 并求得最优解<span class="math inline">\(w^*,b^*\)</span>;</p><ol start="2" type="1"><li>由此得到的分离超平面：</li></ol><p><span class="math display">\[w^*\cdot x + b^*=0\]</span></p><p>​ 分类决策函数 <span class="math display">\[f(x)=sign(w^*\cdot x+b^*)\]</span> <strong>线性可分训练数据集的最大间隔分离朝平面存在唯一的</strong>；</p><h3 id="支持向量和间隔边界">支持向量和间隔边界</h3><p>线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）,支持向量是使 <span class="math display">\[y_i(w\cdot x_i+b)-1=0\]</span> 对<span class="math inline">\(y_i=+1\)</span>的正例点，支持向量在超平面<span class="math inline">\(H_1:w\cdot x +b =1\)</span>;</p><p>对<span class="math inline">\(y_i=-1\)</span>的正例点，支持向量在超平面<span class="math inline">\(H_2:w\cdot x +b =-1\)</span>;</p><p>其中<span class="math inline">\(H_1,H_2\)</span>上的点就是支持向量；</p><p>注意到<span class="math inline">\(H_1,H_2\)</span>平行，没有实例点落在他们中间，它们之间的距离称为间隔，间隔依赖于分离超平面的法向量<span class="math inline">\(w\)</span>,等于<span class="math inline">\(\frac2{||w||}\)</span>,<span class="math inline">\(H_1,H_2\)</span>为间隔边界；<strong>分离超平面只有支持向量起作用，而其他实例点不会影响分离超平面；由于支持向量在确定分离超平面起到决定性的作用，因此这种分类模型叫做支持向量机，支持向量的个数一般很少，所以支持向量由”很少的重要的“训练样本确定；</strong>（支持向量机的命名）</p><p><img src="/280b588e/支持向量.png"></p><h3 id="学习的对偶算法">学习的对偶算法</h3><p>求解线性可分支持向量机的最优化问题(1)可以作为原始最优化问题，应用拉格朗日对偶性可以通过求解对偶问题来得到原始问题的最优解；对偶问题往往更容易求解，而且自然的引入核函数，以便于推广到非线性问题；</p><p>首先构建拉格朗日函数，对于每一个约束条件引入拉格朗日乘子<strong><span class="math inline">\(a_i\geq0,i=1,2,\cdots,N\)</span>（这个将用于后面重新从另一个角度阐述支持向量的关键，注意这个条件）</strong>，则： <span class="math display">\[L(w,b,\alpha)=\frac12||w||^2-\sum\limits^N_{i=1}a_iy_i(w\cdot x_i+b)+\sum^N_{i=1}\alpha_i\]</span> 根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题： <span class="math display">\[\max_\alpha\,\min_{w,b}L(w,b,\alpha)\]</span></p><ol type="1"><li><p>求<span class="math inline">\(\min\limits_{w,b}L(w,b,\alpha)\)</span>，将拉格朗日函数<span class="math inline">\(L(w,b,\alpha)\)</span>分别对<span class="math inline">\(w,b\)</span>,求偏导数并且令其等于0； <span class="math display">\[\nabla_wL(w,b,\alpha)=w-\sum^N_{i=1}a_iy_ix_i = 0\\\nabla_bL(w,b,\alpha)=-\sum^N_{i=1}a_iy_i=0\]</span> 得 <span class="math display">\[w=\sum^N_{i=1}\alpha_iy_ix_i\\\sum^N_{i=1}\alpha_iy_i=0\]</span> 将结论带入拉格朗日函数,得到 <span class="math display">\[L(w,b,\alpha)=\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}a_iy_i((\sum^N_{j=1}\alpha_jy_jx_j)\cdot x_i+b)+\sum^N_{i=1}\alpha_i\\=\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_jx_ix_j-b\sum^N_{i=1}a_iy_i+\sum^N_{i=1}\alpha_i\\=-\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\]</span> 因此求解 <span class="math display">\[\min_{w,b}L(w,b,\alpha)=-\frac12\sum^N_{i=1}\sum^N_{j=1}a_ia_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\]</span></p></li><li><p>对<span class="math inline">\(\min_{w,b}L(w,b,\alpha)\)</span>对<span class="math inline">\(\alpha\)</span>的极大，即是对偶问题 <span class="math display">\[\max_\alpha-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\\s.t.\quad\sum^N_{i=1}\alpha_iy_i=0,\\\alpha_i\geq0,\,i=1,2,\cdots,N\]</span></p></li></ol><p><strong>原始问题满足拉格朗日对偶性中末尾第二个定理的条件，所以存在<span class="math inline">\(w^*,\alpha^*,\beta^*\)</span>,使得<span class="math inline">\(w^*\)</span>是原始问题的解，<span class="math inline">\(\alpha^*,\beta^*\)</span>是对偶问题的解；这就意味着几乎等价的将原始问题转化为对偶问题；</strong>(巧妙~~)</p><hr><p>对线性可分训练数据集，假设对偶最优化问题对<span class="math inline">\(\alpha\)</span>的解为<span class="math inline">\(\alpha^*=(a^*_1,a^*_2,\cdots,a^*_N)^T\)</span>,可以有<span class="math inline">\(\alpha^*\)</span>求得原始最优化问题的解<span class="math inline">\(w^*,b^*\)</span>;i</p><p><strong>定理：</strong> 设<span class="math inline">\(\alpha^*\)</span>是对偶最优化问题的解，则存在下表<span class="math inline">\(j\)</span>,使得<span class="math inline">\(a^*_j&gt;0\)</span>,且 <span class="math display">\[w^*=\sum^N_{i=1}\alpha^*_iy_ix_i\\b^*=y_j-\sum^N_{i=1}\alpha^*_iy_i(x_i\cdot x_j)\]</span></p><p><img src="/280b588e/证明从a求解w,b.png"></p><p>​ 分类决策函数可以写成 <span class="math display">\[f(x)=sign(\sum^N_{i=1}\alpha^*_iy_ix\cdot x_i+b^*)\]</span> 也就是说分裂决策函数只依赖于输入x和训练样本输入的内积；上式称为线性可分支持向量机的对偶形式；</p><p><strong>综上所述，对于给定的线性可分训练数据集，可以先求对偶问题的解<span class="math inline">\(\alpha^*\)</span>在根据定理求解原始问题的<span class="math inline">\(w^*,b^*\)</span>;从而得到分离超平面及分类决策函数；这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机的基本算法；</strong></p><h3 id="线性可分支持向量机学习算法">线性可分支持向量机学习算法</h3><p><img src="/280b588e/线性可分支持向量机学习算法.png"></p><p>线性可分支持向量机中，<span class="math inline">\(w^*,b^*\)</span>只依赖于训练数据中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>,而其他样本点对<span class="math inline">\(w^*,b^*\)</span>没有影响我们将训练数据中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的实例点<span class="math inline">\(x_i\in R^n\)</span>称为支持向量；</p><p><strong>定义：（支持向量）</strong> 考虑最优化问题和对偶最优化问题，将训练数据集中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>的实例<span class="math inline">\(x_i\in R^n\)</span>称为支持向量；</p><p>根据这一定义支持向量一定在间隔边界上，由KKT互补条件可知， <span class="math display">\[a^*_i(y_i(w^*\cdot x_i+b^*)-1)=0,\quad i=1,2,\cdots,N\]</span> 对应于<span class="math inline">\(a^*_i&gt;0\)</span>的实例<span class="math inline">\(x_i\)</span>,有 <span class="math display">\[y_i(w^*\cdot x_i+b^*)-1=0\\w^*\cdot x_i+b^*=\pm1\]</span> 即<span class="math inline">\(x_i\)</span>一定在间隔边界上。这里支持向量的定义与前面给出支持向量是一致的；</p><h1 id="线性支持向量机与软间隔最大化">线性支持向量机与软间隔最大化</h1><h2 id="线性支持向量机">线性支持向量机</h2><blockquote><p>线性可分的问题的支持向量学习方法，对线性不可分的训练数据是不适用的，因为这时上述方法的不等式约束并不都能成立。这就需要修改硬间隔最大化为软间隔最大化；</p></blockquote><p>线性不可分就意味着某些样本点<span class="math inline">\((x_i,y_i)\)</span>不能满足函数间隔大于等于1的约束条件。为了解决这个问题可以对每个样本点<span class="math inline">\((x_i,y_i)\)</span>引入一个松弛变量<span class="math inline">\(\xi_i\geq0\)</span>,使函数间隔加上松弛变量大于等于1，这样约束条件为 <span class="math display">\[y_i(w\cdot x_i +b)\geq 1-\xi _i\]</span> 同时，对每个松弛变量<span class="math inline">\(\xi_i\)</span>，支付一个代价<span class="math inline">\(\xi_i\)</span>,目标函数有原来的<span class="math inline">\(\frac12||w||^2\)</span>变成 <span class="math display">\[\frac12||w||^2+C\sum^N_{i=1}\xi_i\]</span> 这里，<span class="math inline">\(C&gt;0\)</span>称为惩罚参数，一般根据实际问题决定，<em>C</em>值的大小决定了对误分类的惩罚大小；最小化目标函数包含两层含义：使<span class="math inline">\(\frac12||w||^2\)</span>尽量小（间隔尽量大），同时误分类点个数尽量小，<em>C</em>是用来调和二者的系数；</p><p>利用此思路，可以和训练数据线性可分时一样来考虑数据集线性不可分时的线性支持向量机学习问题，称为软间隔最大化；线性不可分的的线性支持向量机学习问题变成了如下凸二次规划问题: <span class="math display">\[\min_{w,b,\xi}\frac12||w||^2+C\sum^N_{i=1}\xi_i\\s.t.\quad y_i(w\cdot x_i+b)\geq1-\xi_i,\quad i=1,2,\cdots,N\\\xi_i\geq,\quad i=1,2,\cdots,N\]</span> 可以证明<em>w</em>是唯一的，但<em>b</em>的解可能不唯一，而是存在一个区间；</p><p><strong>定义（线性支持向量机）：</strong> 对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题，得到分离超平面为 <span class="math display">\[w^*\cdot x+b^*=0\]</span> 以及相应的分类决策函数 <span class="math display">\[f(x)=sign(w^*\cdot x+b^*)\]</span> 称为线性支持向量机；</p><h2 id="学习的对偶算法-1">学习的对偶算法</h2><p>原始最优化问题的拉格朗日函数是 <span class="math display">\[L(w,b,\xi,\alpha,\mu)=\frac12||w||^2+C\sum^N_{i=1}\xi_i-\sum^N_{i=1}\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum^N_{i=1}\mu_i\xi_i\]</span> 其中，<span class="math inline">\(\alpha_i\geq0,\mu_i\geq0\)</span>,对偶问题是拉格朗日极大极小问题，首先对<span class="math inline">\(L(w,b,\xi,\alpha,\mu)\)</span>对<span class="math inline">\(w,b,\xi\)</span>的极小，由 <span class="math display">\[\nabla_wL(w,b,\xi,\alpha,\mu)=w-\sum^N_{i=1}\alpha_iy_ix_i=0\\\nabla_bL(w,b,\xi,\alpha,\mu)=-\sum^N_{i=1}\alpha_iy_i=0\\\nabla_{\xi_i}L(w,b,\xi,\alpha,\mu)=C-\alpha_i-\mu_i=0\]</span> 得到 <span class="math display">\[w=\sum^N_{i-1}\alpha_iy_ix_i\\\sum^N_{i=1}\alpha_iy_i=0\\C-\alpha_i-\mu_i=0\]</span> 将其带入得到 <span class="math display">\[\min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)=-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\]</span> 再对<span class="math inline">\(\min \limits_{w,b,\xi}\,L(w,b,\xi,\alpha,\mu)\)</span>求<span class="math inline">\(\alpha\)</span>的极大，即得对偶问题； <span class="math display">\[\max_\alpha-\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\\s.t.\quad\sum^N_{i=1}\alpha_iy_i=0\\C-\alpha_i-\mu_i=0\\\alpha_i\geq0\\\mu_i\geq 0 ,\quad i=1,2,\cdots,N\]</span> 可以将其转化为 <span class="math display">\[\min_\alpha \frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\alpha_i\\s.t.\quad \sum^N_{i=1}\alpha_iy_i=0\\0\leq\alpha_i\leq C,\quad i=1,2,\cdots,N\]</span> <strong>定理：</strong>设<span class="math inline">\(\alpha^*\)</span>是对偶问题的一个解，若存在<span class="math inline">\(\alpha^*\)</span>的一个分量<span class="math inline">\(\alpha^*,0&lt;\alpha^*_j&lt;C\)</span>,则原始问题的解<span class="math inline">\(w^*,b^*\)</span>,可以按照下式： <span class="math display">\[w^*=\sum^N_{i=1}\alpha^*_iy_ix_i\\b^*=y_j-\sum^N_{i=1}y_i\alpha^*_i(x_i\cdot x_j)\]</span> 证明:</p><p><img src="/280b588e/证明支持向量机求w,b.png"></p><p><img src="/280b588e/证明支持向量机求w,b下.png"></p><p>由此可知，分离超平面 <span class="math display">\[\sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*=0\]</span> 分类决策函数可以写成 <span class="math display">\[f(x)=sign(\sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*)\]</span></p><h2 id="算法">算法</h2><p><img src="/280b588e/线性支持向量机学习算法1.png"></p><p><img src="/280b588e/线性支持向量机学习算法2.png"></p><h2 id="支持向量">支持向量</h2><p>在线性不可分的情况下，将对偶问题的解<span class="math inline">\(\alpha^*\)</span>中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点<span class="math inline">\((x_i,y_i)\)</span>的实例<span class="math inline">\(x_i\)</span>称为支持向量（软间隔的支持向量）。如图</p><p><img src="/280b588e/软间隔的支持向量.png"></p><p>软间隔的支持向量在间隔边界或者分离超平面之间，</p><ul><li>若<span class="math inline">\(\alpha^*_i&lt;C\)</span>,则<span class="math inline">\(\xi_i=0\)</span>,支持向量恰好落在间隔边界上</li><li>若<span class="math inline">\(\alpha^*_i=C,0&lt;\xi_i&lt;1,\)</span>则分类正确，实例落在间隔边界与分离超平面之间</li><li>若<span class="math inline">\(\alpha^*_i=C,\xi_i=1\)</span>,则实例在分离超平面上</li><li>若<span class="math inline">\(\alpha^*_i=C,\xi_i&gt;1\)</span>,则实例点位于分离超平面误分的一侧</li></ul><h2 id="合页损失函数">合页损失函数</h2><p>线性可分支持向量机学习还有另外一种解释，就是最小化以下目标函数 <span class="math display">\[\sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2\]</span> 目标函数的第一项是经验损失或经验风险，函数 <span class="math display">\[L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+\]</span> 称为合页损失函数(hinge loss function)。下标“+”表示一下取正值的函数 <span class="math display">\[[z]_+=\begin{cases}z,z&gt;0\\0,z\leq0\end{cases}\]</span> 这就是说，当样本点<span class="math inline">\((x_i,y_i)\)</span>被正确分类且间隔（置信度）<span class="math inline">\(y_i(w\cdot x_i+b)\)</span>大于1时，损失是0，否则损失是<span class="math inline">\(1-y_i(w\cdot x_i+b)\)</span>.注意到其中有正确分类但损失不是0.目标函数的第二项是函数<span class="math inline">\(\lambda\)</span>的w的<span class="math inline">\(L_2\)</span>范数，是正则化项；</p><p><strong>定理：</strong> 线性支持向量机的原始最优化问题 <span class="math display">\[\min_{w,b,\xi}\frac12||w||^2+C\sum^N_{i=1}\xi\\s.t.\quad y_i(w\cdot x_i +b)\geq1-\xi,\quad i=1,2,\cdots,N\\\xi_i\geq0,\quad i=1,2,\cdots,N\tag{1,2,3}\]</span> 等价于最优化问题 <span class="math display">\[\min_{w,b}\sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2\]</span> <strong>证明:</strong>可以将上面两个式子进行改写，令 <span class="math display">\[[1-y_i(w\cdot x_i+b)]_+=\xi_i\]</span></p><ul><li>由合页函数定义知<span class="math inline">\(\xi_i\geq0\)</span>成立</li><li>当<span class="math inline">\(1-y_i(w\cdot x_i+b)&gt;0\)</span>时，有<span class="math inline">\(1-y_i(w\cdot x_i)=1-\xi_i\)</span></li><li>当<span class="math inline">\(q-y_i(w\cdot x_i+b)\leq0\)</span>时，<span class="math inline">\(\xi_i=0\)</span>,有<span class="math inline">\(y_i(w\cdot x_i+b)\geq1-\xi_i\)</span></li></ul><p>于是，满足约束条件故最优化问题可以写为 <span class="math display">\[\min_{w,b}\sum^N_{i=1}\xi_i+\lambda||w||^2\]</span> 若取<span class="math inline">\(\lambda=\frac1{2C}\)</span>,则 <span class="math display">\[\min_{w,b}\frac1C(\frac12||w||^2+C\sum^N_{i=1}\xi_i)\]</span></p><hr><p>依据下图来做一个简单的总结</p><p><img src="/280b588e/合页损失函数.png"></p><ul><li>0-1损失函数可以认为是二分类问题的真正损失函数，而合页损失函数是其上界；由于0-1损失函数不是连续可导的，直接优化目标函数比较困难，可以认为线性支持向量机是由优化0-1损失函数的上界（合页损失函数）构成的目标函数。此上界又称为代理损失函数；</li><li>图中虚线显示的是感知机的损失函数<span class="math inline">\([-y_i(w\cdot x_i+b)]_+\)</span>,当样本点被正确分类时，损失为0，否则损失为<span class="math inline">\(-y_i(w\cdot x_i+b)\)</span>相比之下，<strong>合页损失函数不仅要求分类正确，而且确信度足够高时损失才是0，合页损失函数有更高的要求</strong>；</li></ul><h1 id="非线性支持向量机与核函数">非线性支持向量机与核函数</h1><blockquote><p>这里介绍说的非线性支持向量机主要特点是利用核技巧（kernel trick），因此会介绍核技巧，其不仅用于支持向量机也用于其他统计学习问题；</p></blockquote><h2 id="核技巧">核技巧</h2><p>一般来说，对给定的一个训练数据集如果一个超曲面能够将正负例正确分开，但可以用一条椭圆曲线（非线性模型）将其分开，则称这个问题为非线性可分问题；</p><p>我们所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题求解原来的非线性问题。通过变换将作图中的椭圆变换成右图中的直线，将非线性问题变换为线性可分类问题；（巧妙）</p><p><img src="/280b588e/非线性分类问题与核技巧示例.png"></p><p>根据上图，用线性分类方法求解线性分类问题可以分成两部</p><ul><li>使用一个变换将空间中的数据映射到新的空间中</li><li>然后再新的空间中使用线性分类学习方法从训练数据中学习分类模型</li></ul><p>核技巧就是这样的方法，核技巧应用支持向量机基本想法是通过一个非线性变换将输入空间（欧式空间<span class="math inline">\(R^b\)</span>或离散集合）对应于一个特征空间（西伯尔特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机来完成；</p><p><strong>核函数的定义</strong>:设输入空间<span class="math inline">\(\chi\)</span>，特征空间<span class="math inline">\(H\)</span>,如果存在一个映射 <span class="math display">\[\phi(x):\chi\rightarrow H\]</span> 使得对所有的<span class="math inline">\(x,z\in\chi,\)</span>函数<span class="math inline">\(K(x,z)\)</span>满足条件 <span class="math display">\[K(x,z)=\phi(x)\cdot\phi(z)\]</span> 则称<span class="math inline">\(K(x,z)\)</span>为核函数，<span class="math inline">\(\phi(x)\)</span>为映射函数，核函数为映射函数的内积；</p><p>核技巧的想法是，在学习与预测中只定义核函数，而不显示的定义映射函数，通常直接定义<span class="math inline">\(K(x,z)\)</span>比较容易，而通过映射函数计算核函数并不容易特征；注意,<strong><span class="math inline">\(\phi\)</span></strong>是输入空间到特征空间的映射，特征空间一般是高维的，甚至是无穷维的，可以看到对给定的核<span class="math inline">\(K(x,z)\)</span>,特征空间和映射函数的取法不唯一，甚至在同意特征空间内也可以取不同的映射；</p><h2 id="核技巧在支持向量中的应用">核技巧在支持向量中的应用</h2><blockquote><p>注意到线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积<span class="math inline">\(x_i\cdot x_j\)</span>,可以用核函数<span class="math inline">\(K(x_i,x_j)\)</span>来代替。</p></blockquote><p>此时对偶问题的目标函数为 <span class="math display">\[W(\alpha)=\frac12\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum^N_{i=1}\alpha_i\]</span> 同样分类决策函数中的内积也可以用核函数代替，因为分类决策函数为 <span class="math display">\[f(x)=sign(\sum^{N_s}_{i=1}\alpha^*_iy_i\phi(x_i)\cdot\phi(x)+b^*)\\=sign(\sum^{N_S}_{i=1}\alpha^*_iy_iK(x_i,x)+b^*)\]</span> 这等价于经过映射函数<span class="math inline">\(\phi\)</span>将原来的输入空间转换到一个新的特征空间，将输入空间中的内积<span class="math inline">\(x_i,x_j\)</span>,变换为特征空间中的内积<span class="math inline">\(\phi(x_i)\cdot\phi(y)\)</span>,在新的特征空间中训练样本中学习线性支持向量机，当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型；</p><p>在核函数给定的条件下可以利用求解线性分类问题的方法求解非线性分类问题；学习是隐式的在特征空间中进行的，不需要显式定义特征空间和映射函数。这称为核技巧；在实际应用中，往往依赖领域知识直接选择核函数，核函数的选择的有效性需要通过实验验证；</p><h2 id="正定核">正定核</h2><p>不构造映射<span class="math inline">\(\phi(x)\)</span>能否判断一个给定的函数<span class="math inline">\(K(x,z)\)</span>是不是核函数？函数<span class="math inline">\(K(x,z)\)</span>满足什么条件才能成为核函数？</p><p>这里叙述正定核的充要条件，通常所说的核函数就是正定核函数（positive definite kernel function)；</p><blockquote><p>假设<span class="math inline">\(K(x,z)\)</span>是定义在<span class="math inline">\(\chi * \chi\)</span>上的对称函数，并且对任意的<span class="math inline">\(x_1,x_2,\cdots,x_m\in\chi ,K(x,z)\)</span>关于<span class="math inline">\(x_1,x_2,\cdots,x_m\)</span>的Gram矩阵是半正定的，可以依据函数<span class="math inline">\(K(x,z)\)</span>,构成一个希尔伯特空间，其步骤为</p><ul><li>先定义映射函数<span class="math inline">\(\phi\)</span>并构成向量空间<em>S</em>;</li><li>然后再<em>S</em>空间上定义内积构成内积空间；</li><li>最后<em>S</em>完备化构成希尔伯特空间</li></ul></blockquote><ol type="1"><li><p>定义映射,构成向量空间<em>S</em></p><p>先定义映射 <span class="math display">\[\phi:x\rightarrow K(\cdot,x)\]</span> 根据这一映射，对任意<span class="math inline">\(x_i\in\chi,\alpha_i\in R,i=1,2,\cdots,m\)</span>,定义线性组合 <span class="math display">\[f(\cdot)=\sum^M_{i=1}\alpha_iK(\cdot,x_i)\]</span> 考虑线性组合为元素的集合<span class="math inline">\(S\)</span>,由于集合<span class="math inline">\(S\)</span>对加法和乘法运算是封闭的，所以<em>S</em>构成一个向量空间；</p></li><li><p>在<em>S</em>上定义内积，使之称为内积空间</p><p>在S上定义运算<span class="math inline">\(*\)</span>；对任意<span class="math inline">\(f,g\in S\)</span>, <span class="math display">\[f(\cdot)=\sum^M_{i=1}\alpha_iK(\cdot,x_i)\\g(\cdot)=\sum^l_{j=1}\beta_jK(\cdot,z_i)\]</span> 定义运算<span class="math inline">\(*\)</span> <span class="math display">\[f*g=\sum^M_{i=1}\sum^L_{j=1}\alpha_i\beta_jK(x_i,z_j)\]</span> <img src="/280b588e/证明定义运算是s的内积_1.png"></p><p><img src="/280b588e/证明定义运算是s的内积_2.png"></p></li><li><p>将内积空间<span class="math inline">\(S\)</span>完备化为希尔伯特空间</p><p><img src="/280b588e/内积空间S完备化.png"></p></li><li><p><strong>正定核的充要条件</strong> 设<span class="math inline">\(K:\chi * \chi\rightarrow R\)</span>是对称函数，则<span class="math inline">\(K(x,z)\)</span>为正定核函数的充要条件是对任意的<span class="math inline">\(x_i\in \chi,i=1,2,\cdots,m,K(x,z)\)</span>对应的Gram矩阵： <span class="math display">\[K=[K(x_i,x_j)]_{m*m}\]</span> 是半正定的；</p><p><strong>定义：（正定核的等价定义)</strong>: 设<span class="math inline">\(\chi\subset R^n，K(x,z)\)</span>是定义在<span class="math inline">\(\chi*\chi\)</span>上的对称函数，对任意的<span class="math inline">\(x_i\in \chi,i=1,2,\cdots,m,K(x,z)\)</span>对应的Gram矩阵 <span class="math display">\[K=[K(x_i,x_j)]_{m*m}\]</span> 是半正定矩阵，则称<span class="math inline">\(K(x,z)\)</span>是正定核；</p><p><strong>此定义在构造核函数时候很有用，但对于一个具体的核函数来说验证它是否为正定核是不容易的，因为要求对任意有限输入集<span class="math inline">\(\{x_1,x_2,\cdots,x_m\}\)</span>验证<em>K</em>对应的Gram矩阵是否为半正定的。实际问题中往往应用已有的核函数。另外，由Mercer定理可以得到Mercer核，正定核比Mercer更具一般性</strong>；</p></li></ol><h2 id="常用核函数">常用核函数</h2><p><img src="/280b588e/常用核函数_1.png"></p><p><img src="/280b588e/常用核函数_2.png"></p><h2 id="非线性支持向量分类机141">非线性支持向量分类机（141）</h2><p>待更新。。。。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 支持向量机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k近邻法</title>
      <link href="//dff65b00/"/>
      <url>//dff65b00/</url>
      
        <content type="html"><![CDATA[<blockquote><p>k近邻算法：</p><p>给定的一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最的k个实例，这k个实例的多数属于某个类，就将该实例划分进入这个类中；(哲学上讲的话，有点近朱者赤近墨者黑的味道~~)</p></blockquote><h2 id="算法核心原理">算法核心原理</h2><ul><li><p>根据给定的<strong>距离度量</strong>，在训练数据集<em>T</em>中找出与<em>x</em>最近邻的<em>k</em>个点，涵盖这<em>k</em>个点的邻域记为</p><p><span class="math inline">\(N_{k}(x)\)</span>;</p></li><li><p>在<span class="math inline">\(N_{k}(x)\)</span>中根据<strong>分类决策规则</strong>决定<span class="math inline">\(x\)</span>的类别<span class="math inline">\(Y\)</span>; <span class="math display">\[Y= arg\max \sum_{x_i\in N_{k}(x)}I(y_{i}=c_{j}),i=1,2 \ldots,N;j=1,2 \ldots,K;\\其中I为指示函数；即y_i=c_j时I为1，否则为I为0；\]</span> <strong><em>k近邻没有显示的学习过程</em></strong></p></li></ul><h2 id="k近邻模型">k近邻模型</h2><p>k近邻算法使用的模型实际上 对应于特征空间的划分；模型有三个基本要</p><ul><li>度量距离</li><li>k值的选择</li><li>分类决策的规则</li></ul><h3 id="距离度量">距离度量</h3><p>将设特征空间<span class="math inline">\(X\)</span>是<span class="math inline">\(n\)</span>维实数向量空间<span class="math inline">\(R^n\)</span>,<span class="math inline">\(x_i,x_j\in X,x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T,x_i,x_j的L_p距离定义为\)</span> <span class="math display">\[L_p(x_I,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|)^{\frac1p}\]</span> 其中<span class="math inline">\(P\geq 1\)</span></p><ul><li><span class="math inline">\(p=1\)</span>称为曼哈顿距离</li><li><span class="math inline">\(p=2\)</span>称为欧氏距离</li><li><span class="math inline">\(p=\infty\)</span>时，他是各个坐标距离的最大值</li></ul><p>如图</p><p><img src="/dff65b00/Lp距离间的关系.png"></p><h3 id="k值的选择">k值的选择</h3><ul><li><p>k值小，预测结果对近邻实例点非常敏感，而且k值减小模型就会变得复杂而且容易发生过拟合；</p></li><li><p>k值大，学习的近似误差会增大，也就意味这模型会变得简单；</p><p><strong>通常采用交叉验证发来选取最优k值</strong></p></li></ul><h3 id="分类决策规则">分类决策规则</h3><p>k近邻法的分类决策规则往往是多数表决；</p><p>如果分类损失函数为0-1损失函数，分类函数为 <span class="math display">\[f:R^n \to \{c_1,c_2,\cdots,c_k\}\]</span> 那么误分类的概率 <span class="math display">\[P(Y\neq f(X)) = 1-P(Y=f(X))\]</span> 对于给定的实例<span class="math inline">\(x\in X\)</span>,其最近邻的<span class="math inline">\(k\)</span>个训练实例点构成的集合<span class="math inline">\(N_k(x)\)</span>,如果覆盖<span class="math inline">\(N_k(x)\)</span>的区域类别为<span class="math inline">\(c_j\)</span>,那么误分类 <span class="math display">\[\frac1k\sum_{x_i\in N_k(x)} = 1-\frac1k\sum_{x_i\in N_k(x)}I(y_i=c_j)\]</span> 使得误分类概率最小就是经验风险最小，也就是<span class="math inline">\(\frac1k\sum_{x_i\in N_k(x)}I(y_i=c_j)\)</span>最大；也就是说多数表决规则等价于经验风险最小化；</p><h2 id="k近邻算法实现kd树">K近邻算法实现：kd树</h2><p>由于实现k近邻算法要对已划分数据进行搜算，数据量大时会耗费大量时间，因此有很多优化提高效率的算法；dk树就是其中的一种</p><h3 id="构造kd树">构造kd树</h3><ol type="1"><li>构造根节点，根节点对应于包含<span class="math inline">\(T\)</span>的k维空间的超矩形区域；</li><li>选择<span class="math inline">\(x^{(1)}\)</span>为坐标轴，以<span class="math inline">\(T\)</span>中所有实例的<span class="math inline">\(x^{(1)}\)</span>坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域；切分由通过切分点并与坐标轴<span class="math inline">\(x^{(1)}\)</span>垂直的超平面实现；左子节点存放对应坐标小于切分点的点、右边存放大于切分点的，根节点保存出与切分平面上的点；</li><li>重复：对于深度为j的节点，选择<span class="math inline">\(x^{(l)}\)</span>为切分的坐标轴，<span class="math inline">\(l=j(mod\,k)+1\)</span>,以该节点的区域中所有实例的<span class="math inline">\(x^{(l)}\)</span>坐标的中位数为切分点，再次进行与（2）中类似的划分；</li><li>直到两个子区域没有实例存在时停止，从而形成<span class="math inline">\(kd\)</span>树的区域划分；</li></ol><p>图示如下</p><p><img src="/dff65b00/kd树示例.png"></p><h3 id="搜索kd树">搜索kd树</h3><p>包含目标点的叶节点对应的包含目标点的最小超矩形区域。以此叶节点的实例点作为当前最近点。目标点的最邻近点一定在以目标点为中心并通过当前最近点的超群体内部。然后返回当前节点的父节点，如果父节点的另一子节点的超矩形区域与超球体相交，那么在相交区域内寻找与目标结点最近的点。如果存在，将此作为新的最近点。算法转到更上一级父节点继续上述过程；如果父节点另一子节点的超矩形区域与超球体不相交，或不存在比当前最近点更近点则停止操作；</p><ol type="1"><li>在kd树中找出包含目标点x的叶节点：从根节点出发递归的向下方位，直到子节点为叶节点为止；</li><li>以此叶节点为“当前最近点”；</li><li>递归的向上回退，在每个节点执行下述操作<ul><li>如果该节点保存的实例点比当前最近点距离目标更近，则该实例点为“当前最近点”；</li><li>当前最近点一定位于该节点一个子节点对应的区域，检查该子节点的父节点的另一个子节点对应的区域是否有更近的点，有则确认零一点为最近点----具体可检查另一子节点对应区域是否以目标点为球心、以当前最小距离为半径的超球体相交；</li><li>不相交则向上回退；</li></ul></li><li>回退到根节点，搜索结束；</li></ol><p>图示如下</p><p><img src="/dff65b00/kd树搜索.png"></p><p><strong><span class="math inline">\(kd\)</span>树搜索平均计算复杂度是<span class="math inline">\(O(log\,N)\)</span>,<span class="math inline">\(N\)</span>为训练实例数。kd树适合用于训练实例数远大于空间维数时的k紧邻搜索。二者接近时，<span class="math inline">\(kd\)</span>树效率会迅速下降，几乎线性扫描；</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> k近邻 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="//8ddc7426/"/>
      <url>//8ddc7426/</url>
      
        <content type="html"><![CDATA[<h1 id="核心原理">核心原理</h1><blockquote><p>可以将决策树看成是一个if-then规则的集合，决策树的路径或其对应的if-then规则集合<strong>互斥且完备</strong>；每一个实例都被一条路径或一条规则覆盖，而且只被一条路径或规则覆盖；</p></blockquote><p>决策树与条件概率分布示意图</p><p><img src="/8ddc7426/决策树对应条件概率分布.png"></p><p><strong>决策树学习的目标</strong></p><p>根据给定的训练数据集构建一个决策树模型（学习），并使他们能够对实例进行正确的分类(泛 化)；</p><p><strong>决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化</strong>；损失函数确定后，学习问题就变成了损失函数意义下选择最优决策树的问题；由于从所有决策树中寻找最优决策树是np完全问题，所以常常使用启发式的算法来近似求解这一最优化问题；以此寻找到次最优的；</p><hr><p><strong>决策树学习的核心</strong></p><ul><li>特征选择——（选取核心数据）</li><li>决策树生成——（构建决策树）——局部最优</li><li>决策树剪枝——（提高泛化能力）——全局最优</li></ul><h1 id="特征选择">特征选择</h1><h2 id="信息增益">信息增益</h2><p><strong>信息增益是衡量特征选择的一个重要指标；</strong>用表示训练数据集对应某个特征所具备的分类能力；</p><h3 id="熵">熵</h3><blockquote><p>信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为 <span class="math display">\[P(X=x_i)=p_i,i=1,2,\cdots,n\]</span> 则随机变量的熵定义为 <span class="math display">\[H(X)=-\sum\limits^n_{i=1}p_i\log p_i\\其中定义：0\log0=0\]</span> 其中对数常取2为底或者e为底，这时熵的单位分别是比特或纳特。从定义可知熵只依赖于X的分布，而与X的取值无关，所以也将X的熵就做<span class="math inline">\(H(p)\)</span>;熵越大，随机变量的不确定性就越大，且有定义可知 <span class="math display">\[0\leq H(p)\leq\log n（基本不等式可推导）\]</span> p_当随机变量只有两个取值0，1时，熵为 <span class="math display">\[H(p)=-p\log_2p-(1-p)\log_2(1-p)\]</span> 这时熵<span class="math inline">\(H(p)\)</span>随概率<span class="math inline">\(p\)</span>变化的曲线如图所示</p><p><img src="/8ddc7426/熵与概率变化的曲线.png"></p><p>条件熵<span class="math inline">\(H(Y|X)\)</span>表示在随机变量X已知的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望 <span class="math display">\[H(Y|X)=\sum\limits_{i=1}^np_iH(Y|X=x_i),其中p_i=P(X=x_i),i=1,2,\cdots,n\]</span></p><hr><p>当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验上和经验条件熵</p></blockquote><p><strong>信息增益表示得知特征X的信息儿时的类Y的信息不确定性减少的程度</strong></p><p>定义：特征A对训练数据集D的信息增益g(D,A),集合D的经验熵<span class="math inline">\(H(D)\)</span>与特征A给定条件下D的经验条件熵<span class="math inline">\(H(D|A)\)</span>之差 <span class="math display">\[g(D,A)=H(D)-H(D|A)\]</span></p><h3 id="算法">算法</h3><ol type="1"><li>计算数据集D的经验熵<span class="math inline">\(H(D)\)</span></li></ol><p><span class="math display">\[H(D)=-\sum\limits_{k=1}^k\frac{|C_K|}{|D|}\log_2\frac{|C_K|}{|D|}\]</span></p><ol start="2" type="1"><li><p>计算特征A对数据集D的经验条件熵 <span class="math display">\[H(D|A)= \sum^n_{i=1}\frac{|D_i|}{|D|}H(D_i)\]</span></p></li><li><p>计算信息增益</p></li></ol><p><span class="math display">\[g(D,A) = H(D)-H(D|A)\]</span></p><h3 id="信息增益比">信息增益比</h3><p>信息增益划分数据及特征存在偏向于选择取值较多的特征的问题。使用信息增益比可以对此作出矫正</p><p>信息增益比定义： <span class="math display">\[g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\其中，H_A(D)=-\sum^n_{i=1}\frac{|D_i|}{|D|}\log_2\frac{|Di|}{|D|}\]</span></p><h1 id="决策树生成">决策树生成</h1><h2 id="id3算法">ID3算法</h2><ol type="1"><li>若D中所有的实例属于同一类，则T为单节点树，并将类<span class="math inline">\(C_k\)</span>作为该节点的类标记，返回T;</li><li>若<span class="math inline">\(A=\emptyset\)</span>,则T为单节点树，并将D中的实例树最大的类<span class="math inline">\(C_K\)</span>作为该节点的类标记，返回T;</li><li>否则，计算A中各特征对D的信息增益，选择信息增益最大的特征<span class="math inline">\(A_g\)</span>;</li><li>如果<span class="math inline">\(A_g\)</span>的信息增益小于阀值，则置T为单节点树，并将D中实例数最大的类<span class="math inline">\(C_k\)</span>作为标记，返回T；</li><li>否则，对于<span class="math inline">\(A_g\)</span>中的每一可能值<span class="math inline">\(a_i\)</span>,依<span class="math inline">\(A_g=a_i\)</span>,将D分割为若干非空子集<span class="math inline">\(D_i\)</span>，将<span class="math inline">\(D_I\)</span>中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T;</li><li>对第i个子节点，以<span class="math inline">\(D_i\)</span>为训练集，以<span class="math inline">\(A-\{A_g\}\)</span>为特征集，递归的调用步骤1~5，得到子树<span class="math inline">\(T_i\)</span>,返回<span class="math inline">\(T_i\)</span>;</li></ol><h2 id="c4.5的生成算法">C4.5的生成算法</h2><p>与ID3算法相似，C4.5在生成的过程中，用信息增益比来选择特征；</p><h1 id="决策树剪枝">决策树剪枝</h1><h2 id="原理">原理</h2><p>递归生成的决策树往往对训练数据分类准确但是对未知数据的分类没有那么准确，会出现过拟合现象，因此需要从已经生成的树上裁掉一些子树或叶节点，从而简化分类树模型；</p><p><strong>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现；</strong>设树T的叶节点个数为<span class="math inline">\(|T|\)</span>,t是树T的叶节点，该叶节点有<span class="math inline">\(N_t\)</span>个样本点，其中k类的样本点有<span class="math inline">\(N_{tk}\)</span>个，<span class="math inline">\(k=1,2,\cdots,K,H_t(T)\)</span>为叶节点上的经验熵，<span class="math inline">\(\alpha\geq0\)</span>为参数，则决策树损失函数定义为 <span class="math display">\[C_a(T)=\sum\limits^{|T|}_{t=1}{N_tH_t(T)+\alpha|T|}\\其中经验熵为:H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\\简写为：C_\alpha(T)=C(T)+\alpha|T|\]</span> <span class="math inline">\(C(T)\)</span>表示模型对讯联数据的预测误差，|T|表示模型复杂程度，<span class="math inline">\(\alpha\)</span>用来控制模型复杂度和准确度之间的影响；</p><h2 id="算法-1">算法</h2><ol type="1"><li>计算出每个节点的经验熵；</li><li>递归的从树的叶节点向上回缩；设一组叶节点回缩到其父节点之前与之后的整体树分别为<span class="math inline">\(T_B\)</span>与<span class="math inline">\(T_A\)</span>,其对应的损失函数值分别为<span class="math inline">\(C_a(T_B)\)</span>与<span class="math inline">\(C_a(T_A)\)</span>，如果<span class="math inline">\(C_a(T_A)\leq C_a(T_B)\)</span>则进行剪枝，将父节点变为新的叶节点；</li><li>返回2，知道不能继续为止，得到损失函数最小的子树<span class="math inline">\(T_\alpha\)</span>;</li></ol><p><strong>注意，只需要考虑两个数的损失函数的差，其计算可以再局部进行。所以决策树的剪枝算法可以由一种动态规划算法来实现</strong></p><h1 id="cart算法">CART算法</h1><h2 id="cart生成">CART生成</h2><h3 id="回归树">回归树</h3><p>递归的构建二叉树，对回归树用平方误差最小化准则，对分类树使用基尼指数最小化准则，生成二叉树；</p><p>定义两个区域： <span class="math display">\[R_1(j,s)=\{x|x^{(j)}\leq s\}，R_2(j,s)=\{x|x^{(j)}&gt; s\}，\]</span> 寻找最优切分变量<span class="math inline">\(j\)</span>和最优切分点<span class="math inline">\(s\)</span>,即求解 <span class="math display">\[\min\limits_{j,s}[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}{(y_i-c_1)^2} +\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}{(y_i-c_2)^2} ]\]</span> 对固定的输入变量<span class="math inline">\(j\)</span>可以找到最优切分点<span class="math inline">\(s\)</span> <span class="math display">\[\hat c_1 = ave(y_i|x_i\in R_1(j,s)),\hat c_2 = ave(y_i|x_i\in R_2(j,s))\]</span> 遍历所有输入变量，找到切分点构成一对<span class="math inline">\((j,s)\)</span>.依此将输入空间划分成两个区域，紧接着重复划分，到满足停止条件；这样就得到了一棵回归树，这样的称为最小二乘回归树；</p><p><strong>算法</strong></p><ol type="1"><li>选择最优切分变量<span class="math inline">\(j\)</span>与切分点<span class="math inline">\(s\)</span>,求解</li></ol><p><span class="math display">\[\min\limits_{j,s}[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}{(y_i-c_1)^2} +\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}{(y_i-c_2)^2} ]\]</span></p><p>找到使之最小的一组解<span class="math inline">\((j,s)\)</span></p><ol start="2" type="1"><li>用选定的对<span class="math inline">\((j,s)\)</span>划分区域并决定对应的输出值：</li></ol><p><span class="math display">\[\hat c_m=\frac1{N_m}\sum\limits_{s_i\in R_m(j,x)}y_i,\,x\in R_m,\,\,m=1,2\]</span></p><ol start="3" type="1"><li>继续对两个区域调用步骤1,2，直至满足停止条件</li><li>将输入控件划分成<span class="math inline">\(M\)</span>个区域，生成决策树；</li></ol><p><span class="math display">\[f(x)=\sum\limits^M_{m=1}\hat c_mI(x\in R_m)\]</span></p><h3 id="分类树">分类树</h3><p><strong>基尼指数定义</strong></p><p>分类问题中，假设有k个类，样本点属于第k类的概率为概率为<span class="math inline">\(p_k\)</span>,对于给定的样本集合<span class="math inline">\(D\)</span>,其基尼指数为 <span class="math display">\[Gini(p)=1-\sum\limits^K_{k=1}p^2_k\]</span> 在特征A的条件下，集合D的基尼指数定义为 <span class="math display">\[Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\]</span> 基尼指数<span class="math inline">\(Gini(D)\)</span>表示集合<span class="math inline">\(D\)</span>的不确定性，基尼指数<span class="math inline">\(Gini(D,A)\)</span>表示经过类<span class="math inline">\(A=a\)</span>分割后集合<span class="math inline">\(D\)</span>的不确定性；基尼指数越大，不确定性就越大</p><p><img src="/8ddc7426/基尼指数_熵_分类误差率.png"></p><p><strong>算法</strong></p><ol type="1"><li><p>设节点的训练数据集为<span class="math inline">\(D\)</span>,计算现有特征对该数据及的基尼指数。对每一个特征<span class="math inline">\(A\)</span>,其可能的每个取值<span class="math inline">\(a\)</span>根据样本点对<span class="math inline">\(A=a\)</span>测试为“是”或”否“将<span class="math inline">\(D\)</span>分割为<span class="math inline">\(D_1,D_2\)</span>两部分,计算<span class="math inline">\(A=a\)</span>时<span class="math inline">\(Gini(D,A)\)</span>；</p></li><li><p>在所有可能的特征<span class="math inline">\(A\)</span>以及它们所有可能的切分点<span class="math inline">\(a\)</span>中选择基尼指数最小的特征及其切分点；并生成两个子节点。将现有的数据集依据特征分配到两个子节点中；</p></li><li><p>对两个子节点递归的调用1,2;</p></li><li><p>生成CART决策树；</p></li></ol><h2 id="cart剪枝">CART剪枝</h2><p><strong>原理</strong></p><p>定义子树的损失函数： <span class="math display">\[C_\alpha(T)=C(T)+\alpha|T|\]</span> 具体的，从整体<span class="math inline">\(T_0\)</span>,开始剪枝，对<span class="math inline">\(T_0\)</span>的任意内部节点<span class="math inline">\(t\)</span>,以<span class="math inline">\(t\)</span>为单节点树的损失函数为 <span class="math display">\[C_\alpha(t) = C(t)+\alpha\]</span> 以<span class="math inline">\(t\)</span>为根节点的子树<span class="math inline">\(T_t\)</span>的损失函数为 <span class="math display">\[C_\alpha(T_t)=C(T_t)+\alpha|T_t|\]</span> 通过上面式易得，<span class="math inline">\(\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}\)</span>为剪枝的零界点，此时虽然二者有相同的损失函数，但<span class="math inline">\(t\)</span>比<span class="math inline">\(T_t\)</span>结点少，因此应该剪枝；</p><p>为此对<span class="math inline">\(T_0\)</span>中的每一个内部节点<span class="math inline">\(t\)</span>计算 <span class="math display">\[g(t)=\frac{C(t)-C(T_t)}{|T_t-1|}\]</span> 表示剪枝后整体损失函数减小的程度.在<span class="math inline">\(T_0\)</span>中减去<span class="math inline">\(g(t)\)</span>最小的<span class="math inline">\(T_t\)</span>,将得到的子树作为<span class="math inline">\(T_t\)</span>,将得到的子树作为<span class="math inline">\(T_1\)</span>，同时将最小的<span class="math inline">\(g(t)\)</span>设为<span class="math inline">\(\alpha_1\)</span>.<span class="math inline">\(T_1\)</span>为区间<span class="math inline">\([\alpha_1,\alpha_2)\)</span>的最优子树。如此不断剪枝直到根节点；期间不断产生新的<span class="math inline">\(\alpha\)</span>,并产生新的区间；</p><p><strong>在得到的子树序列中使用交叉验证发求得最优子树<span class="math inline">\(T_\alpha\)</span></strong></p><p>具体的，利用独立的验证数据集，测试子树序列中的各个子树的平方误差或基尼指数。寻找对应指数最小的子树，同时子树确定也就意味着独影的<span class="math inline">\(\alpha_k\)</span>确定；</p><hr><p><strong>算法</strong></p><ol type="1"><li><p>设<span class="math inline">\(k=0,T=T_0\)</span>;</p></li><li><p>设<span class="math inline">\(\alpha=+\infty;\)</span></p></li><li><p>自上而下的对各内部节点<span class="math inline">\(t\)</span>计算<span class="math inline">\(C(T_1),|T_t|\)</span>以及</p></li></ol><p><span class="math display">\[g(t)=\frac{C(t)-C(T_t)}{|T_t|-1} \,,\alpha=\min(\alpha,g(t))\]</span></p><p>这里，<span class="math inline">\(T_t\)</span>表示以<span class="math inline">\(t\)</span>为根节点的子树，<span class="math inline">\(C(T_t)\)</span>是对训练数据集的预测误差，<span class="math inline">\(|T_t|\)</span>是<span class="math inline">\(T_t\)</span>的叶节点个数；</p><ol start="4" type="1"><li>对<span class="math inline">\(g(t)=\alpha\)</span>的内部节点<span class="math inline">\(t\)</span>进行剪枝，并对叶节点<span class="math inline">\(t\)</span>以多数表决的方法确定其类，得到树<span class="math inline">\(T\)</span>;</li><li>设<span class="math inline">\(k=k+1,\alpha_k=\alpha,T_k=T\)</span>;</li><li>如果<span class="math inline">\(T_k\)</span>不是由根节点及两个叶节点构成的树，则回退到步骤2；否则令<span class="math inline">\(T_k=T_n\)</span>;</li><li>采用交叉验证法在子树序列<span class="math inline">\(T_0,T_1,\cdots,T_n\)</span>中选取最优子树<span class="math inline">\(T_\alpha\)</span>;</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯</title>
      <link href="//f77f0066/"/>
      <url>//f77f0066/</url>
      
        <content type="html"><![CDATA[<h2 id="数学基础">数学基础</h2><p>先验概率： 根据以往的经验分析得到的概率；</p><p>后验概率：根据结果推导原因；</p><p>贝叶斯公式：</p><p><img src="/f77f0066/贝叶斯公式.png"></p><h2 id="核心原理">核心原理</h2><p>朴素贝叶斯法通过训练数据集学习联合概率分布<span class="math inline">\(P(X,Y)\)</span>，具体的学习先验概率分布 <span class="math display">\[P(Y=c_k),k=1,2,\cdots,K\]</span> 条件概率分布 <span class="math display">\[P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\cdots,K\]</span> 于是学习到联合概率分布<span class="math inline">\(P(X,Y)\)</span></p><p>实际上，朴素贝叶斯对条件概率分布做了<strong>条件独立性的假设</strong>，这一假设使得朴素贝叶斯发相对简单；但也会<strong>牺牲一定的分类准确率</strong>；</p><hr><p>朴素贝叶斯法分类时，对于给定的输入x，通过学习到的模型计算后验概率分布，将后验概率最大的类作为x的输出类。具体的朴素贝叶斯分类器可以表示为 <span class="math display">\[y=f(x)=arg\max\limits_{c_k} \frac{P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}\]</span> 由于上式中所有分母都是相同的，因此上式等价于 <span class="math display">\[y=arg\max \limits_{c_k} P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)\]</span></p><hr><p><strong>后验概率最大化等价于经验风险最小化</strong>（后验概率最大化的含义）</p><p><img src="/f77f0066/后验概率最大化含义.png"></p><h2 id="朴素贝叶斯的参数估计">朴素贝叶斯的参数估计</h2><h3 id="极大似然估计">极大似然估计</h3><ol type="1"><li><p>计算先验概率及条件概率 <span class="math display">\[P(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)}{N},k=1,2,\cdots,K\]</span></p><p><span class="math display">\[P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_j=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}\\j=1,2,\cdots,n;l=1,2,\cdots,S_j;k=1,2,\cdots,K\]</span></p></li><li><p>对于给定的实例<span class="math inline">\(x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T\)</span>,计算 <span class="math display">\[p(Y=c_k)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,\cdots,K\]</span></p></li><li><p>确定实例x的类 <span class="math display">\[y=arg\max\limits_{c_k}P(Y=c_k)\prod\limits^n_{j=1}P(X^{j}=x^{j}|Y=c_k)\]</span></p></li></ol><h3 id="贝叶斯估计">贝叶斯估计</h3><p>极大似然估计可能会出现要估计的概率值为0的情况，这会影响到后验概率的计算结果。使分类产生偏差。可以用贝叶斯估计来解决这一问题；具体的</p><hr><p>先验概率的贝叶斯估计是 <span class="math display">\[P_\lambda(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}\]</span> 条件概率的贝叶斯估计是 <span class="math display">\[P\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum\limits^{S_j}_{i=1}I(y_i=c_k)+S_j\lambda}\]</span></p><hr><p>其中，<span class="math inline">\(\lambda\geq0\)</span>,相当于在随机变量各个取值上赋予一个整数<span class="math inline">\(\lambda&gt;0\)</span>.当<span class="math inline">\(\lambda=0\)</span>时就是极大似然估计；常取<span class="math inline">\(\lambda=1\)</span>,这时称为拉普拉斯平滑。显然 <span class="math display">\[P_\lambda(X^{(j)}=a_{jl}|Y=c_k)&gt;0\\ \sum\limits^{S_j}_{j=1}P(X^{(j)}=a_{jl}|Y=c_k)=1\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 攻克&#39;小蓝书&#39; </tag>
            
            <tag> 原理 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
