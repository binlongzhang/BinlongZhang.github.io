<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico">
  <link rel="mask-icon" href="/images/logo.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"binlongzhang.github.io","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":"ture","color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Basic  Text to Sequence">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP技术演进">
<meta property="og:url" content="https://binlongzhang.github.io/63784/index.html">
<meta property="og:site_name" content="Gonlnib&#39;s Blog">
<meta property="og:description" content="Basic  Text to Sequence">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080010418-893ce74d-586f-4eaa-95bf-2d9d16834b49.png?x-oss-process=image%252Fresize%252Cw_1500">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080103891-a0d24135-bab9-4c28-8b56-412302662de5.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/jpeg/1455047/1610071796564-54fb2269-79b4-423b-9821-1c4214e2c46d.jpeg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/f4b3b6c1c603e6ffb626b2e6effa689a.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609079440694-b12ffd5c-f080-452f-90f0-803dfaae26d7.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/8dec559e201a7b6a0f99baeaa1731051.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609989398334-53ac2b94-1f98-49ba-89f8-827d4f12e59a.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080488164-d9af00c7-8973-4bf6-a4c4-b67d53b2cf32.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954395936-45591b69-375e-4a69-ae3e-dd56ffca7527.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954402153-4420b575-615e-4b13-b6ae-23d40ad25bce.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954591387-9b5e9185-4bed-4989-98fa-9d84df0c7736.png?x-oss-process=image%252Fresize%252Cw_1500">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954771257-073fa0ec-9e23-4400-9afc-b1f7620b09cf.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954799006-1d566531-ad02-4ea7-b8a1-2b2858d9321a.png?x-oss-process=image%252Fresize%252Cw_1500">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609088933301-0e5a3546-5eb9-4a9d-94cc-8c43c11167c1.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609089435257-dafeb65b-26f3-40d3-b9a3-26e427422c38.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609091143755-730b9da7-6471-42bc-b2a3-1774bf72f4d8.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609091387248-dac39a01-98db-49a5-81fe-d9c307207d67.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/5ebea707e73f58aeb693a6839f40beaa.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/b2236e0874d0f2ae943cd6bd45752b03.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/6c4ff69dbcc329835a33b80fe3a145c7.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/8933246d7ae375bdc2c53e80e3117668.svg">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609094216617-902eb5da-ed60-4f44-be7a-cf71960b5d0b.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609094677279-7bdfb17d-2020-4743-a262-34d96c42ad04.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609095598284-033853e6-23b7-42d5-93e8-9d0eb316eb09.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609095880134-7cebc46d-e79e-47c8-b404-d359fb66601b.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609096282427-9acf7924-facf-405b-8e74-38eb52e656c2.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609305830020-9f73a8fb-a455-4c3e-b282-f5e4e38c205c.png?x-oss-process=image%252Fresize%252Cw_1500">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609098189979-9ac67b25-ebba-4f7c-aeb6-6870f63c2907.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609098285697-5bbf6e32-2ae0-460a-81be-acd8d335f5d2.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609306899667-27815155-0c01-4d26-8d53-874f6f1c8b38.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609307173567-48346fb3-8e87-40c8-9d1d-8bed2578e179.png?x-oss-process=image%252Fresize%252Cw_1500">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099126607-977ff213-390a-4f28-8549-c97e7f178e23.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099241741-649cf726-3b7e-439c-8edc-8a5a18277fcf.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099828373-d5964209-8c59-408e-ae25-6686897bdcf4.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609307579370-7b5ec29c-5cd1-404e-b05d-0a97ea5bbd88.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099879166-9667fa64-9e31-4234-9b9d-a70a044939cc.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099930635-09e26044-a7e8-4353-9c62-c909406463d7.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1611633512066-0c621534-a717-4c0e-b3b0-04a4343493bd.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/1455047/1611650633282-c539fe36-5d1d-46b5-a3ad-baa6e4c0512d.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120458752-d041688c-4758-4994-b2d2-46946626b99d.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120471126-16ce1732-46c7-4260-89d3-7b39ea9a9e6c.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120751686-b54847a3-41f6-4542-bde0-8518bd6c7501.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120872166-204f1218-fdc9-4ee2-a227-1cae3792dab8.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120931792-e22d7cf2-838b-412a-b827-cb04cbea7778.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609121849418-cf59238d-9936-49ee-9abe-5eaef870da82.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609123908355-6b8ddc02-3620-4776-af18-4e2dc236d6c2.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124093451-a851c90a-36ec-4163-a57c-3d0536b39736.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124247557-0e41f23a-902b-40cd-9731-b54ca3fc3919.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124304800-44bd498b-6c30-4c7a-8867-4d901c4caca1.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124350349-6886f2ea-1153-400e-973e-22df53a73945.png">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124403751-24ad5f62-b899-44af-9b78-6bcb2d62521d.png">
<meta property="article:published_time" content="2021-04-04T03:10:55.000Z">
<meta property="article:modified_time" content="2021-04-04T03:12:40.095Z">
<meta property="article:author" content="Gonlnib">
<meta property="article:tag" content="技术演进">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080010418-893ce74d-586f-4eaa-95bf-2d9d16834b49.png?x-oss-process=image%252Fresize%252Cw_1500">

<link rel="canonical" href="https://binlongzhang.github.io/63784/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>NLP技术演进 | Gonlnib's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Gonlnib's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">17</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">9</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://binlongzhang.github.io/63784/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="Gonlnib">
      <meta itemprop="description" content="Gonlnib 的个人博客，主要内容会涉及一些学习记录、思考、理解，也会有一些个人爱好的展示">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gonlnib's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP技术演进
        </h1>

        <div class="post-meta">

          
          	<i class="fa fa-thumb-tack"></i>
          	<font color=7D26CD>置顶</font>
          	<span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-04-04 11:10:55 / 修改时间：11:12:40" itemprop="dateCreated datePublished" datetime="2021-04-04T11:10:55+08:00">2021-04-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
          <br>
            <span id="/63784/" class="post-meta-item leancloud_visitors" data-flag-title="NLP技术演进" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/63784/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/63784/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.3k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="basic"><a class="markdownIt-Anchor" href="#basic"></a> Basic</h1>
<h2 id="text-to-sequence"><a class="markdownIt-Anchor" href="#text-to-sequence"></a> Text to Sequence</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080010418-893ce74d-586f-4eaa-95bf-2d9d16834b49.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080103891-a0d24135-bab9-4c28-8b56-412302662de5.png" alt="image.png">**<br>
**</p>
<p><strong>分词的算法大致分为两种：</strong></p>
<p>1.基于词典的分词算法</p>
<p>正向最大匹配算法 逆向最大匹配算法 双向匹配分词法</p>
<p>2.基于统计的机器学习算法</p>
<p>N-gram、HMM、CRF、SVM、LSTM+CRF</p>
<p><strong>jieba分词的框架图</strong></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/1455047/1610071796564-54fb2269-79b4-423b-9821-1c4214e2c46d.jpeg" alt="image"></p>
<h2 id="word-embedding"><a class="markdownIt-Anchor" href="#word-embedding"></a> Word Embedding</h2>
<ol>
<li>
<p><strong>First</strong>,represent words using one-hot vectors.</p>
</li>
<li>
<ol>
<li>Suppose the dictionary contain V unique words;</li>
<li>then the one-hot vectors <img src="https://cdn.nlark.com/yuque/__latex/f4b3b6c1c603e6ffb626b2e6effa689a.svg" alt="img"> are v-dimensional;</li>
</ol>
</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609079440694-b12ffd5c-f080-452f-90f0-803dfaae26d7.png" alt="image.png"></p>
<ol>
<li>
<p><strong>second</strong>,map the one-hot vectors to low-dimensional vectors</p>
</li>
<li>
<ol>
<li>P is parameter matrix which can be <strong>learned from training data;</strong></li>
<li><img src="https://cdn.nlark.com/yuque/__latex/8dec559e201a7b6a0f99baeaa1731051.svg" alt="img">is the one-hot vector of the i-th word in dictonary;</li>
</ol>
</li>
</ol>
<p><strong>How to interpret the parameter matrix?</strong></p>
<p><strong><img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609989398334-53ac2b94-1f98-49ba-89f8-827d4f12e59a.png" alt="image.png"></strong></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609080488164-d9af00c7-8973-4bf6-a4c4-b67d53b2cf32.png" alt="image.png"></p>
<h3 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h3>
<blockquote>
<p>无监督训练<strong>word2vec</strong>的两种模型：CBOW和skip-gram</p>
</blockquote>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954395936-45591b69-375e-4a69-ae3e-dd56ffca7527.png" alt="14.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954402153-4420b575-615e-4b13-b6ae-23d40ad25bce.png" alt="15.png"></p>
<h3 id="tip17png"><a class="markdownIt-Anchor" href="#tip17png"></a> Tip<img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954591387-9b5e9185-4bed-4989-98fa-9d84df0c7736.png?x-oss-process=image%2Fresize%2Cw_1500" alt="17.png"></h3>
<p>如果一个语料库稍微大一些，可能的结果简直太多了，最后一层相当于softmax，计算起来十分耗时，有什么办法来解决嘛？</p>
<ul>
<li>输入两个单词，看他们是不是前后对应的输入和输出，也就相当于一个二分类任务<img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954771257-073fa0ec-9e23-4400-9afc-b1f7620b09cf.png" alt="19.png"></li>
<li>出发点非常好，但是此时训练集构建出来的标签全为1，无法进行较好的训练</li>
<li>改进方案：加入一些负样本（<strong>负采样模型</strong>）<img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1609954799006-1d566531-ad02-4ea7-b8a1-2b2858d9321a.png?x-oss-process=image%2Fresize%2Cw_1500" alt="21.png"></li>
</ul>
<h1 id="use-word-classification"><a class="markdownIt-Anchor" href="#use-word-classification"></a> Use word Classification</h1>
<p><strong>至此就可以尝试使用各种分类器对齐进行分类,</strong></p>
<h3 id="shortcoming"><a class="markdownIt-Anchor" href="#shortcoming"></a> ShortComing</h3>
<p>仅仅是利用词的统计和频率分类，没有考虑序列；</p>
<h1 id="rnnrecurrent-neural-networks"><a class="markdownIt-Anchor" href="#rnnrecurrent-neural-networks"></a> RNN(Recurrent Neural Networks)</h1>
<blockquote>
<p>using RNN to instead simple Classification</p>
</blockquote>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609088933301-0e5a3546-5eb9-4a9d-94cc-8c43c11167c1.png" alt="image.png"></p>
<h2 id="simple-rnn"><a class="markdownIt-Anchor" href="#simple-rnn"></a> Simple RNN</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609089435257-dafeb65b-26f3-40d3-b9a3-26e427422c38.png" alt="image.png"></p>
<p>注意上述结构中的<strong>双曲正切函数</strong>是必要的，用于防止传递过程中发生的A的n次方导致后方序列失去效果！</p>
<h3 id="shortcoming-2"><a class="markdownIt-Anchor" href="#shortcoming-2"></a> ShortComing</h3>
<ul>
<li>SimpleRNN is good at short-term dependence;</li>
<li>SimpleRNN is bad at long-term dependence;</li>
<li>Only one such parameter matrix,no matter how long the sequence is;</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609091143755-730b9da7-6471-42bc-b2a3-1774bf72f4d8.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609091387248-dac39a01-98db-49a5-81fe-d9c307207d67.png" alt="image.png"></p>
<h2 id="lstm-modelusing-lstm-instead-simple-rnn"><a class="markdownIt-Anchor" href="#lstm-modelusing-lstm-instead-simple-rnn"></a> LSTM Model(Using LSTM instead Simple RNN)</h2>
<blockquote>
<p>Hochreiter and Schmidhuber. Long short-term memory. Neural computation, 1997.</p>
</blockquote>
<ul>
<li>
<p><strong>Conveyor belt</strong>: <strong>the past information directly flows to the future.(解决梯度消失)</strong></p>
</li>
<li>
<p>Each of the following blocks has a parameter matrix:</p>
</li>
<li>
<ul>
<li><strong>Forget gate</strong>**😗*forget the gender of the old subject.</li>
<li><strong>Input gate</strong>:decides which values of the conveyor belt we’ll update.</li>
<li><strong>New value (</strong><img src="https://cdn.nlark.com/yuque/__latex/5ebea707e73f58aeb693a6839f40beaa.svg" alt="img"><strong>)</strong>: to be added to the conveyor belt</li>
<li><strong>Output gate</strong>**😗*decide what flows from the conveyor belt<img src="https://cdn.nlark.com/yuque/__latex/b2236e0874d0f2ae943cd6bd45752b03.svg" alt="img">to the state <img src="https://cdn.nlark.com/yuque/__latex/6c4ff69dbcc329835a33b80fe3a145c7.svg" alt="img">.</li>
</ul>
</li>
<li>
<p>Number of parameters:</p>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/__latex/8933246d7ae375bdc2c53e80e3117668.svg" alt="img"></p>
<p><strong>Gate Struct</strong></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609094216617-902eb5da-ed60-4f44-be7a-cf71960b5d0b.png" alt="image.png"></p>
<p><strong>Update</strong></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609094677279-7bdfb17d-2020-4743-a262-34d96c42ad04.png" alt="image.png"></p>
<h2 id="rnn-more-effective"><a class="markdownIt-Anchor" href="#rnn-more-effective"></a> RNN More Effective</h2>
<h3 id="stacked-rnn"><a class="markdownIt-Anchor" href="#stacked-rnn"></a> Stacked RNN</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609095598284-033853e6-23b7-42d5-93e8-9d0eb316eb09.png" alt="image.png"></p>
<h3 id="bidirectional-rnn"><a class="markdownIt-Anchor" href="#bidirectional-rnn"></a> Bidirectional RNN</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609095880134-7cebc46d-e79e-47c8-b404-d359fb66601b.png" alt="image.png"></p>
<h3 id="pretraining"><a class="markdownIt-Anchor" href="#pretraining"></a> Pretraining</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609096282427-9acf7924-facf-405b-8e74-38eb52e656c2.png" alt="image.png"></p>
<h1 id="seq2seq"><a class="markdownIt-Anchor" href="#seq2seq"></a> Seq2Seq</h1>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609305830020-9f73a8fb-a455-4c3e-b282-f5e4e38c205c.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<p>How to Improve?</p>
<ul>
<li>Bi-LSTM instead of LSTM;(Encoder ONLY!)</li>
<li>Tokenization in the word-level(instead of char-level);</li>
<li>Multitask learning;</li>
<li>Attention</li>
</ul>
<h1 id="attention"><a class="markdownIt-Anchor" href="#attention"></a> Attention</h1>
<blockquote>
<p>Bahdanau,Cho,&amp; Bengio. Neural	machine	translation by jointly learning to align and translate. In ICLR, 2015.</p>
</blockquote>
<ul>
<li>Attention tremendously improves Seq2Seq model.</li>
<li>With attention, Seq2Seq model does not forget source input.</li>
<li>With attention, the decoder knows where to focus.</li>
<li>Downside: much more computation.</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609098189979-9ac67b25-ebba-4f7c-aeb6-6870f63c2907.png" alt="image.png"></p>
<p><strong>Much ways to calculate Weight:</strong></p>
<p><strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609098285697-5bbf6e32-2ae0-460a-81be-acd8d335f5d2.png" alt="image.png"></strong></p>
<p><strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609306899667-27815155-0c01-4d26-8d53-874f6f1c8b38.png" alt="image.png"></strong></p>
<p>**<br>
**<strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609307173567-48346fb3-8e87-40c8-9d1d-8bed2578e179.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></strong></p>
<p>**<br>
**</p>
<p><strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099126607-977ff213-390a-4f28-8549-c97e7f178e23.png" alt="image.png"></strong></p>
<p>**<br>
**</p>
<p><strong><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099241741-649cf726-3b7e-439c-8edc-8a5a18277fcf.png" alt="image.png"></strong></p>
<h1 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> Self-Attention</h1>
<p>通用性更强，跳出了seq to seq 的局限；</p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099828373-d5964209-8c59-408e-ae25-6686897bdcf4.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609307579370-7b5ec29c-5cd1-404e-b05d-0a97ea5bbd88.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099879166-9667fa64-9e31-4234-9b9d-a70a044939cc.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609099930635-09e26044-a7e8-4353-9c62-c909406463d7.png" alt="image.png"></p>
<h1 id="attention-to-seq2seq"><a class="markdownIt-Anchor" href="#attention-to-seq2seq"></a> Attention to seq2seq</h1>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1611633512066-0c621534-a717-4c0e-b3b0-04a4343493bd.png" alt="image.png"></p>
<h1 id="transformer"><a class="markdownIt-Anchor" href="#transformer"></a> Transformer</h1>
<blockquote>
<ol>
<li><strong>Bahdanau, Cho, &amp; Bengio.</strong> Neural machine translation by jointly learning to align and<br>
translate. <strong>In ICLR, 2015.</strong></li>
<li><strong>Cheng, Dong, &amp; Lapata.</strong> Long Short-Term Memory-Networks for Machine Reading. <strong>In</strong><br>
<strong>EMNLP, 2016.</strong></li>
<li><strong>Vaswani et al.</strong> Attention Is All You Need**. In NIPS, 2017.**</li>
</ol>
</blockquote>
<ul>
<li>Transformer is a Seq2Seq model.</li>
<li>Transformer is not RNN.</li>
<li>Purely based attention and dense layers.</li>
<li>Higher accuracy than RNNs on large datasets.</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/1455047/1611650633282-c539fe36-5d1d-46b5-a3ad-baa6e4c0512d.png" alt="image.png"></p>
<h2 id="attention-without-rnn"><a class="markdownIt-Anchor" href="#attention-without-rnn"></a> Attention without RNN</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120458752-d041688c-4758-4994-b2d2-46946626b99d.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120471126-16ce1732-46c7-4260-89d3-7b39ea9a9e6c.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120751686-b54847a3-41f6-4542-bde0-8518bd6c7501.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120872166-204f1218-fdc9-4ee2-a227-1cae3792dab8.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609120931792-e22d7cf2-838b-412a-b827-cb04cbea7778.png" alt="image.png"></p>
<h2 id="self-attention-without-rnn"><a class="markdownIt-Anchor" href="#self-attention-without-rnn"></a> Self-Attention without RNN</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609121849418-cf59238d-9936-49ee-9abe-5eaef870da82.png" alt="image.png"></p>
<h2 id="transformer-2"><a class="markdownIt-Anchor" href="#transformer-2"></a> Transformer</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609123908355-6b8ddc02-3620-4776-af18-4e2dc236d6c2.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124093451-a851c90a-36ec-4163-a57c-3d0536b39736.png" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124247557-0e41f23a-902b-40cd-9731-b54ca3fc3919.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124304800-44bd498b-6c30-4c7a-8867-4d901c4caca1.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124350349-6886f2ea-1153-400e-973e-22df53a73945.png" alt="image.png"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/1455047/1609124403751-24ad5f62-b899-44af-9b78-6bcb2d62521d.png" alt="image.png"></p>
<ul>
<li>Transformer is Seq2Seq model; it has an encoder and a decoder.</li>
<li>Transformer model is not RNN.</li>
<li>Transformer is based on attention and self-attention.</li>
<li>Transformer outperforms all the state-of-the-art RNN models.</li>
</ul>
<h2 id="bertbidirectional-encoder-representationsfrom-transformers"><a class="markdownIt-Anchor" href="#bertbidirectional-encoder-representationsfrom-transformers"></a> BERT(Bidirectional Encoder Representationsfrom Transformers)</h2>
<blockquote>
<p>\1. Devlin, Chang, Lee, and Toutanova. BERT: Pre-training of deep bidirectional transformers for language</p>
<p>understanding. In ACL, 2019.</p>
<p>\2. Vaswani and others. Attention is all you need. In NIPS, 2017.</p>
</blockquote>
<p><strong>Main:</strong></p>
<ul>
<li>Predict masked word.</li>
<li>Predict next sentence.</li>
</ul>
<p><strong>Combining the two methods</strong></p>
<ul>
<li>Loss 1 is for binary classification (i.e., predicting the nextsentence.)</li>
<li>Loss 2 and Loss 3 are for multi-class classification (i.e., predicting the masked words.)</li>
<li>Objective function is the <strong>sum of the three loss functions.</strong></li>
<li><strong>Update model parameters</strong> by performing one gradient descent.</li>
</ul>
<p><strong>Data</strong></p>
<ul>
<li>BERT does not need manually labeled data. (Nice! Manual labeling is expensive.)</li>
<li>Use large-scale data, e.g., English Wikipedia (2.5 billion words.)</li>
<li>Randomly mask words (with some tricks.)</li>
<li>50% of the next sentences are real. (The other 50% are fake.)</li>
</ul>

    </div>

    
    
    

    <div>
      
        ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>
        <div class="reward-container">
  <div>作者将会持续总结、分享，如果愿意支持作者，作者将会十分感激！！！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Gonlnib 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Gonlnib 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>





      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B/" rel="tag"><i class="fa fa-tag"></i> 技术演进</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/11849/" rel="prev" title="文本预处理-TextToVector">
      <i class="fa fa-chevron-left"></i> 文本预处理-TextToVector
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#basic"><span class="nav-text"> Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#text-to-sequence"><span class="nav-text"> Text to Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word-embedding"><span class="nav-text"> Word Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-text"> Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tip17png"><span class="nav-text"> Tip</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#use-word-classification"><span class="nav-text"> Use word Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#shortcoming"><span class="nav-text"> ShortComing</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnnrecurrent-neural-networks"><span class="nav-text"> RNN(Recurrent Neural Networks)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-rnn"><span class="nav-text"> Simple RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#shortcoming-2"><span class="nav-text"> ShortComing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lstm-modelusing-lstm-instead-simple-rnn"><span class="nav-text"> LSTM Model(Using LSTM instead Simple RNN)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn-more-effective"><span class="nav-text"> RNN More Effective</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#stacked-rnn"><span class="nav-text"> Stacked RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bidirectional-rnn"><span class="nav-text"> Bidirectional RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pretraining"><span class="nav-text"> Pretraining</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#seq2seq"><span class="nav-text"> Seq2Seq</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention"><span class="nav-text"> Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#self-attention"><span class="nav-text"> Self-Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-to-seq2seq"><span class="nav-text"> Attention to seq2seq</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transformer"><span class="nav-text"> Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#attention-without-rnn"><span class="nav-text"> Attention without RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention-without-rnn"><span class="nav-text"> Self-Attention without RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-2"><span class="nav-text"> Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bertbidirectional-encoder-representationsfrom-transformers"><span class="nav-text"> BERT(Bidirectional Encoder Representationsfrom Transformers)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gonlnib"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">Gonlnib</p>
  <div class="site-description" itemprop="description">Gonlnib 的个人博客，主要内容会涉及一些学习记录、思考、理解，也会有一些个人爱好的展示</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">gonlnib</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">54k</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>









         <div class=BbeiAn-info" style="color:black pos" align="center">
                  陕ICP备 -
                  <a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">20001965号</a>
                  </a>
         </div>

      </div>


    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='1' zIndex='-1' count='199' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@latest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail',];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'RmyI9HlT5Oh3mf6JFv93EYol-gzGzoHsz',
      appKey     : 'IArUETpHH06QrO50jMFkGho5',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
