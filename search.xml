<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>支持向量机</title>
      <link href="/archives/280b588e.html"/>
      <url>/archives/280b588e.html</url>
      
        <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>支持向量机（support vector machines,SVM）是一种二分类模型；它的基本模型定义在特征空间上的间隔最大模型分类器，间隔最大使之区别于感知机；同时还可以使用核技巧，使它成为非线性支持向量机；SVM的学习策略就是间隔最大化，等价于正则化的合页函数最小化；支持向量机的学习算法就是求解凸二次规划的最优化算法；</p><p>由简至繁的模型分别为</p><ul><li>线性可分支持向量机--硬间隔最大化</li><li>线性支持向量机--软间隔最大化</li><li>非线性支持向量机--核技巧+软间隔最大化</li></ul><h1 id="线性可分支持向量机">线性可分支持向量机</h1><p>线性可分支持向量机、线性支持向量机假设输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；支持向量机的学习实在特征空间内进行的；</p><p>假设训练数据集是线性可分的，且对于样本点<span class="math inline">\((x_i,y_i)\)</span>,当<span class="math inline">\(y_i=+1\)</span>时<span class="math inline">\(x_i\)</span>为正例；当<span class="math inline">\(y_i=-1\)</span>时<span class="math inline">\(x_i\)</span>为负例；</p><p>学习的目标是找到分离超平面<span class="math inline">\(w\cdot x+b=0,\quad w\)</span>是法向量，<span class="math inline">\(b\)</span>为截距，法向量指向的是正类；一般的，训练数据集线性可分时存在无穷多个分离超平面将两类数据正确划分。感知机利用误分类最小策略，求得分离超平面；线性可分支持向量机利用间隔最大化求最优分离超平面；</p><p><strong>定义</strong>：（线性可分支持向量机）给定线性可分数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为 <span class="math display">\[w^*\cdot x+b^*=0\]</span> 相应的决策函数为 <span class="math display">\[f(x)=sign(w^*\cdot x+b^*)\]</span></p><h2 id="函数间隔和几何间隔">函数间隔和几何间隔</h2><p><strong>定义</strong>：</p><blockquote><p>(函数间隔)给定训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,关于样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔为 <span class="math display">\[\hat\gamma_i=y_i(w\cdot x_i+b)\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[\hat \gamma= \min_{i=1,\cdots,N}\hat \gamma_i\]</span></p></blockquote><p>函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔是不够的；如果成比例改变w和b，超平面没有改变但是间隔却变为原来的两倍；</p><p>因此我们可以对分离超平面的法向量加某些约束，如规范化，<span class="math inline">\(||w||=1\)</span>,使得间隔是确定的，这时函数间隔为几何间隔 <span class="math display">\[\gamma_i=\frac w{||w||}\cdot x_i+\frac b{||w||}\]</span> <img src="/archives/几何间隔.png"></p><p>定义</p><blockquote><p>（几何间隔）对于给定的训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((w,b)\)</span>,定义超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔为 <span class="math display">\[\gamma_i=y_i(\frac w{||w||}\cdot x_i+\frac b{||w||})\]</span> 定义超平面<span class="math inline">\((w,b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((w,b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((x_i,y_i)\)</span>的函数间隔最小值， <span class="math display">\[\gamma= \min_{i=1,\cdots,N} \gamma_i\]</span></p></blockquote><p>超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i,y_i)\)</span>的几何间隔一般是实例点到朝平面的带符号距离，正确分类时就是实例点到超平面的距离；</p><p>函数间隔和几何间隔有如下关系 <span class="math display">\[\gamma_i=\frac{\hat \gamma_i}{||w||}\\\gamma=\frac{\hat \gamma}{||w||}\]</span> 如果<span class="math inline">\(||w||=1\)</span>,那么函数间隔和几何间隔相等；</p><h2 id="间隔最大化">间隔最大化</h2><p><span class="math inline">\(a&#39;\)</span></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>logistic回归和最大熵模型</title>
      <link href="/archives/7e7ef195.html"/>
      <url>/archives/7e7ef195.html</url>
      
        <content type="html"><![CDATA[<h2 id="logistic回归模型">logistic回归模型</h2><h3 id="logistic分布">logistic分布</h3><p><strong>定义logistic分布</strong> <span class="math display">\[F(x)=P(X\leq x)=\frac1{1+e^{-(x-\mu)/\gamma}}\\f(x)=F^{&#39;}(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\mu})^2}\]</span> 分布函数图像是一条S形曲线，以点<span class="math inline">\((\mu,\frac12)\)</span>为中心对称，如下图</p><p><img src="/archives/logistic分布.png"></p><h3 id="二项logistic回归模型">二项logistic回归模型</h3><p><strong>定义二项logistic是如下条件概率分布</strong> <span class="math display">\[p(Y=1|x)=\frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)}\\p(Y=0|x)=\frac1{1+\exp(w\cdot x+b)}\]</span> 为了方便有时将偏置并入权重中，即<span class="math inline">\(w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^T,x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)\)</span>,同时<strong><em>logistic</em></strong>模型简写为 <span class="math display">\[p(Y=1|x)=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\p(Y=0|x)=\frac1{1+\exp(w\cdot x)}\]</span> 定义对数几率（log odds)或<strong><em>logit</em></strong>函数为 <span class="math display">\[logit(p)=log\frac p{1-p}\]</span> 对logistc回归而言 <span class="math display">\[log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x\]</span> 说明了在<em>logistic</em>回归中，输出<span class="math inline">\(Y=1\)</span>的对数概率是输入<span class="math inline">\(x\)</span>的线性函数；通过<em>logistic</em>回归定义模型可以将线性函数<span class="math inline">\(w\cdot x\)</span>转换为概率；</p><h3 id="模型的参数估计">模型的参数估计</h3><p>对于给定训练数据集可以应用极大似然估计法估计模型参数，</p><p>设 <span class="math display">\[P(Y=1|x)=\pi(x),\,\,\,P(Y=1|x)=1-\pi(x)\]</span> 似然函数为 <span class="math display">\[\prod^N_{i=1}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\]</span> 对数似然函数 <span class="math display">\[L(w)=\sum\limits ^N_{i=1}[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))]\\=\sum\limits^N_{i=1}[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i)]\]</span> 对<span class="math inline">\(L(w)\)</span>求极大值，得到<span class="math inline">\(w\)</span>的估计值；至此通常采用梯度下降法或者拟牛顿法来求解；</p><h3 id="多项logistic回归">多项logistic回归</h3><p>将其从二项推广到多项<em>logistic</em>回归模型 <span class="math display">\[P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum\limits^{k-1}_{k=1}\exp(w_k\cdot x)},k=1,2,\cdots,K-1\\P(Y=K|x)=\frac1{1+\sum\limits^{k-1}_{k=1}\exp(w_k\cdot x)}\]</span> 同理，二项<em>logistic</em>回归的参数估计法也可以推广到多项<em>logistic</em>回归；</p><h2 id="最大熵模型">最大熵模型</h2><h3 id="原理">原理</h3><p>最大熵模型是概率模型学习的一个准则，最大熵原理认为，在所有可能的概率模型中，熵最大的是最好的模型，通常用约束条件确定概率模型的集合；设离散变量<span class="math inline">\(X\)</span>分布是<span class="math inline">\(P(X)\)</span>，熵为 <span class="math display">\[H(P)=-\sum\limits_xP(x)\log P(x)\]</span> 且满足不等式 <span class="math display">\[0\leq H(P) \leq \log|X|\]</span> 直观的，在没有更多信息的情况下认为不确定部分是等可能的，“等可能”不易操作，而熵则是一个可优化的指标以此来达到等可能的目的；</p><p><img src="/archives/最大熵原理的概率模型解释.png"></p><h3 id="最大熵模型定义">最大熵模型定义</h3><p>对于给定训练数据集 <span class="math display">\[\hat P(X=x,Y=y)=\frac{v(X=x,Y=y)}N\\\hat P(X=x)=\frac{v(X=x)}N\\其中v(X=x,Y=y)表示样本(x,y)出现的频数；\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>定义 <span class="math display">\[f(x)=\left\{ \begin{aligned} 1,&amp;\,\,x与y满足某一事实\\0,&amp;\,否则\end{aligned}\right.\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>关于经验分布<span class="math inline">\(\hat P(X,Y)\)</span>的期望值表示为 <span class="math display">\[E_\hat p(f)=\sum_{x,y}\hat P(x,y)f(x,y)\]</span> 特征函数<span class="math inline">\(f(x,y)\)</span>关于模型<span class="math inline">\(P(Y|X)\)</span>与经验分布<span class="math inline">\(\hat P(X)\)</span>的期望值 <span class="math display">\[E_ p(f)=\sum_{x,y}\hat P(x)P(y|x)f(x,y)\]</span> 如果模型能够获取训练数据中的信息，那么可以假设这两个期望值相等 <span class="math display">\[E_p(f)=E_\hat p(f)\]</span> <strong>假设满足所有约束条件的模型集合为</strong> <span class="math display">\[C\equiv\{P\in p|E_P(f_i)=E_\hat p(f_i),i=1,2,\cdots,n\}\]</span> 定义在条件概率分布<span class="math inline">\(P(Y|X)\)</span>上的条件熵为 <span class="math display">\[H(P)=-\sum\limits_{x,y}\hat p(x)P(y|x)\log P(y|x)\]</span> 则模型集合<span class="math inline">\(C\)</span>中条件熵<span class="math inline">\(H(P)\)</span>最大的模型为最大熵模型，其中的对数为自然对数；</p><h3 id="最大熵模型的学习">最大熵模型的学习</h3><p>最大熵模型的学习可以形式化为约束最优化问题；</p><p>对于给定的训练数据集<span class="math inline">\(T\)</span>以及特征函数<span class="math inline">\(f_i(x,y),i=1,2,\cdots,n\)</span>,最大熵模型的学习等价于约束最优化问题 <span class="math display">\[\max\limits_{p\in C}\,H(P)=-\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)\\s.t.\,\,\,\,\,E_p(f_i)=E_{\hat p}(f_i),\,i=1,2,\cdots,n\\\sum_yP(y|x)=1\]</span> 按照习惯可以将其等价的改为最小值问题 <span class="math display">\[\max\limits_{p\in C}\,H(P)=-\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)\\s.t.\,\,\,\,\,E_p(f_i)=E_{\hat p}(f_i),\,i=1,2,\cdots,n\\\sum_yP(y|x)=1\]</span> 求约束最优化问题就是求解最大熵模型。可以将约束最优化问题转化为无约束最优化的对偶问题。通过求解对偶问题来求解原始问题；</p><hr><p><strong>具体推导</strong></p><p>引入拉格朗日乘子<span class="math inline">\(w_0,w_1,\cdots,w_n\)</span>,定义拉格朗日函数<span class="math inline">\(L(P,w)\)</span>; <span class="math display">\[L(P,w)\equiv-H(P)+w_0(1-\sum\limits_yP(y|x))+\sum\limits^n_{i=1}w_i(E_\hat p(f_i)-E_p(f_i))\\=\sum\limits_{x,y}\hat P(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x))+\sum^n_{i=1}w_i(\sum\limits_{x,y}\hat P(x,y)f_i(x,y)-\sum_{x,y}\hat P(x)P(y|x)f_i(x,y))\]</span> 最优化原始问题 <span class="math display">\[\min_{p\in C} \max_wL(P,w)\]</span> 对偶问题 <span class="math display">\[\max_w\min_{P\in C}L(P,w)\]</span> 由于拉格朗日函数<span class="math inline">\(L(P,w)\)</span>是<span class="math inline">\(P\)</span>的凸函数，原始问题与对偶问题是等价的（对偶问题的定理)首先记作 <span class="math display">\[\psi(w)=\min_{p\in C}L(P,w)=L(P_w,w)\]</span> <span class="math inline">\(\psi(w)\)</span>称为对偶函数，同时，其解记作 <span class="math display">\[P_w=\arg\min_{p\in C}L(P,w)=P_w(y|x)\]</span> 具体的求<span class="math inline">\(L(P,w)\)</span>对<span class="math inline">\(P(y|x)\)</span>的偏导数，令偏导数等于0，解得 <span class="math display">\[P(y|x)=\exp(\sum_{i=1}^nw_if_i(x,y)+w_0-1)=\frac{\exp(\sum\limits_{i=1}^nw_if_i(x,y))}{\exp(1-w_0)}\]</span> 由于<span class="math inline">\(\sum\limits_yP(y|x)=1\)</span>,得 <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum\limits^n_{i=1}w_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span> 之后求解外部的极大化问题 <span class="math display">\[\max\limits_w\psi(x)\]</span> 记其解为<span class="math inline">\(w^*\)</span>, <span class="math display">\[w^*=\arg\max\limits_w\psi(w)\]</span></p><h3 id="极大似然估计">极大似然估计</h3><p>对偶函数的极大化等价于最大熵模的极大似然估计；</p><p>证明过程：</p><p><img src="/archives/对偶函数极大化等价于最大熵模型极大似然估计.png"></p><p>这样最大熵模型的学习问题就可以转换为具体的求解对数似然函数极大化或对偶函数极大化的问题；写成更一般的形式 <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span></p><h3 id="深入理解">深入理解</h3><p>最大熵模型与<em>logistic</em>回归模型有着类似的形式，它们又称为对数线性模型（log linear model）,模型学习就是在给定的训练数据集条件下对模型进行极大似然估计或正则化的极大似然估计；</p><hr><p>事实上，定义特征函数，其中<span class="math inline">\(g(x)\)</span>为提取出每个x的特征，，输出是<span class="math inline">\(x\)</span>的特征向量： <span class="math display">\[\left\{\begin{aligned} &amp;g(x),y=1\\&amp;0,\quad y=0\end{aligned}\right.\]</span> 将以上特征带入到最大熵模型中 <span class="math display">\[P(y=1|x)=\frac{\exp(w_ig(x))}{\exp(w_ig(x))+\exp(w_i*0)}\]</span> 上下同时除<span class="math inline">\(\exp(w_ig(x))\)</span>,得 <span class="math display">\[P(y=1|x)=\frac1{1+\exp(-w_ig(x))}\]</span> 同理 <span class="math display">\[P(y=0|x)=\frac{\exp(w_i\cdot0)}{\exp(w_ig(x))+\exp(w_i*0)}\\=\frac1{\exp(w_ig(x))+1}\]</span> 自然的发现<em>logistic</em>回归模型其实就是最大熵模型在<span class="math inline">\(y=1\)</span>时抽取x的特征这一情况；之前我们用极大似然估计求参数<span class="math inline">\(w_i\)</span>其实这样求出的模型就是<span class="math inline">\(\max P_w(y|x)\)</span>,所以就是求最大熵模型；</p><p>日常生活中，我们经常不知不觉的就是用了最大熵模型，这里给出了更高层面的抽象的最大熵模型；显然最后的例子也说明了<span class="math inline">\(logistic\)</span>回归其实也是一种最大熵模型；</p><h2 id="模型的最优化算法">模型的最优化算法</h2><p>逻辑斯蒂回归模型、最大熵模型归结为似然函数为莫表的最优化问题，通常通过迭代算法求解，从最优化的角度上来看这时的目标函数具有很好的性质，他是光滑的凸函数；因此多种最优化方法都适用；常用的方法有改进的迭代尺度法、梯度下降法、牛顿法、拟牛顿法。牛顿法或拟牛顿法；牛顿法或者拟牛顿法一般收敛速度更快；</p><p><strong>最大熵模型</strong> <span class="math display">\[P_w(y|x)=\frac1{Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y))\\其中，\,\,\,Z_w(x)=\sum\limits_y\exp(\sum^n_{i=1}w_if_i(x,y))\]</span> 对数似然函数 <span class="math display">\[L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}w_if_i(x,y)-\sum_x\hat P(x)\log Z_w(x)\]</span></p><h3 id="改进的迭代尺度算法iis">改进的迭代尺度算法(IIS)</h3><h4 id="原理-1">原理</h4><p><strong>IIS</strong>核心想法是：建设最大熵模型当前的参数向量是<span class="math inline">\(w=(w_1,w_2,\cdots,w_n)^T\)</span>,我们希望找到一个新的参数向量<span class="math inline">\(w+\delta=(w_1+\delta_1,w_2+\delta_2,\cdots,w_n+\delta_n)\)</span>,使得模型的对数似然函数值增大。如果能有一种参数更新方法让<span class="math inline">\(w\rightarrow+\delta\)</span>,那么重复使用即可找到对数似然函数的最大值；</p><p>对于给定的经验分布<span class="math inline">\(\hat P(x,y)\)</span>,对数似然函数的该变量是 <span class="math display">\[L(w+\delta)-L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)-\sum_x\hat P(x)\log\frac{Z_{w+\delta}(x)}{Z_w(x)}\]</span></p><p>利用不等式 <span class="math display">\[-\log\alpha\geq1-\alpha,\alpha&gt;0\]</span> 则 <span class="math display">\[-\sum\limits_x\hat P(x)\log\frac{Z_{w+\delta}(x)}{Z_w(x)}\\\geq\sum_x\hat P(x)(1-\frac{Z_{w+\delta}(x)}{Z_w(x)})\\\geq\sum_x\hat P(x)-\sum_x\hat P(x)\frac{Z_{w+\delta}(x)}{Z_w(x)}\\\geq1-\sum_x\hat P(x)\frac{Z_{w+\delta}(x)}{Z_w(x)}\]</span></p><p><span class="math display">\[L(w+\delta)-L(w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\exp(\sum^n_{i=1}\delta_if_i(x,y))\]</span> 右端记为<span class="math inline">\(A(\delta|w)\)</span>，于是 <span class="math display">\[L(w+\delta)-L(w)\geq A(\delta|w)\]</span> 如果能找到合适的<span class="math inline">\(\delta\)</span>使得下界<span class="math inline">\(A(\delta|w)\)</span>提高，那么对数似然函数也会提高；然而，函数其中的遍量<span class="math inline">\(\delta\)</span>是一个向量含有多个变量，不易同时优化。IIS试图一次只优化其中一个变量<span class="math inline">\(\delta_i\)</span>,而固定其他变量；</p><p>为此引入一个新的量 <span class="math display">\[f^\#(x,y)=\sum\limits_if_i(x,y)\]</span> 于是 <span class="math display">\[A(\delta|w)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\exp(f^\#(x,y)\sum^n_{i=1}\frac{\delta_if_i(x,y))}{f^\#(x,y)}\]</span> 由于<span class="math inline">\(\frac{f_i(x,y)}{f^\#(x,y)}\geq0\)</span>且<span class="math inline">\(\sum\limits^n_{i=1}\frac{f_i(x,y)}{f^\#(x,y)}=1\)</span>,根据<span class="math inline">\(Jensen\)</span>不等式，得到 <span class="math display">\[\exp(\sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\delta_if^\#(x,y))\leq \sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\exp(\delta_if^\#(x,y))\]</span> 记<span class="math inline">\(A(\delta|x)\)</span>改写后的为<span class="math inline">\(B(\delta|x)\)</span> <span class="math display">\[B(\delta|x)=\sum\limits_{x,y}\hat P(x,y)\sum^n_{i=1}\delta_if_i(x,y)+1-\sum_x\hat P(x)\sum_yP_w(y|x)\sum^n_{i=1}\frac{f_i(x,y))}{f^\#(x,y)}\exp(\delta_if^\#(x,y))\]</span> 于是 <span class="math display">\[L(w+\delta)-L(w)\geq B(\delta|w)\]</span> 显然其是对数似然函数的一个新的下界，求<span class="math inline">\(B(\delta|w)\)</span>对<span class="math inline">\(\delta_i\)</span>的偏导数，并令其为0得到 <span class="math display">\[\sum_{x,y}\hat P(x)P_w(y|x)f_i(x,y)\exp(\delta_i,f^\#(x,y))=E_\hat P(f_i)\]</span> 依次对其求解可算出<span class="math inline">\(\delta\)</span>;</p><h4 id="算法">算法</h4><p><em>input:</em>特征函数<span class="math inline">\(f_1,f_2,\cdots,f_n\)</span>,经验分布<span class="math inline">\(\hat P(X,Y)\)</span>,模型<span class="math inline">\(P_w(y|x)\)</span></p><p><em>output:</em>最优参数值<span class="math inline">\(w_i^*\)</span>;最优模型<span class="math inline">\(P_w\)</span></p><ol type="1"><li><p>对所有的<span class="math inline">\(i\in \{1,2,\cdots,n\}\)</span>,取初值<span class="math inline">\(w_i=0\)</span></p></li><li><p>对每一<span class="math inline">\(i\in \{1,2,\cdots,n\}\)</span></p><ol type="1"><li>令<span class="math inline">\(\delta_i\)</span>是方程</li></ol><p><span class="math display">\[\sum_{x,y}\hat P(x)P_w(y|x)f_i(x,y)\exp(\delta_i,f^\#(x,y))=E_\hat P(f_i)\\其中，f^\#(x,y)=\sum\limits_if_i(x,y)\]</span></p><p>的解；</p><ol start="2" type="1"><li>更新<span class="math inline">\(w_i\)</span>值：<span class="math inline">\(w_i\leftarrow w_i+\delta_i\)</span></li></ol></li><li><p>若不是所有的<span class="math inline">\(w_i\)</span>都收敛，重复2</p></li></ol><hr><p>这一算法的关键一步就是2.1,求解其中的<span class="math inline">\(\delta_i\)</span>,如果<span class="math inline">\(f^\#(x,y)\)</span>是常数，则可以显示的表示为 <span class="math display">\[\delta_i=\frac1M\log \frac{E_\hat p(f_i)}{E_p(f_i)}\]</span> 若<span class="math inline">\(f^\#(x,y)\)</span>不是常数，那么必须通过数值计算<span class="math inline">\(\delta_i\)</span>，简单有效的方法就是拟牛顿法；</p><p>以<span class="math inline">\(g(\delta_i)=0\)</span>表示2.1中的方程，牛顿法通过迭代求得的<span class="math inline">\(\delta^*_i\)</span>,使得<span class="math inline">\(g(\delta_i^*)=0\)</span>,迭代公式 <span class="math display">\[\delta_i^{(k+1)}=\delta_i^{(k)}-\frac{g(\delta_i^{(k)})}{g^{&#39;}(\delta_i^{(k)})}\]</span> 只要适当的选取初始值<span class="math inline">\(\delta_i^{(0)}\)</span>,由于<span class="math inline">\(\delta_i\)</span>的方程有单根，因此牛顿法恒收敛，而且收敛速度很快；</p><h3 id="拟牛顿法">拟牛顿法</h3><p>对于最大熵模型而言</p><hr><p><span class="math display">\[P_w(y|x)=\frac{\exp(\sum\limits_{i=1}^nw_if_i(x,y))}{\sum\limits_y\exp(\sum\limits_{i=1}^nw_if_i(x,y))}\\\]</span></p><p>目标函数(极大化似然函数就等价于) <span class="math display">\[\min\limits_{w\in R^n}\quad f(w)=\sum_x\hat P(x)\log\sum_y\exp(\sum^n_{i=1}w_if_i(x,y))-\sum_{x,y}\hat P(x,y)\sum^n_{i=1}w_if_i(x,y)\]</span> 梯度 <span class="math display">\[g(w)=(\frac{\partial f(w)}{\partial w_1},\frac{\partial f(w)}{\partial w_2},\cdots,\frac{\partial f(w)}{\partial w_n})^T\]</span> 其中 <span class="math display">\[\frac{\partial f(w)}{\partial w_i}=\sum\limits_{x,y}\hat P(x)P_w(y|x)f_i(x,y)-E_{\hat P}(f_i),\quad i=1,2,\cdots,n\]</span></p><hr><p>最大熵模型学习的BFGS算法</p><p><em>input</em>:特征函数<span class="math inline">\(f_1,f_2,\cdots,f_n\)</span>;经验分布<span class="math inline">\(\hat P(x,y)\)</span>,目标函数<span class="math inline">\(f(w)\)</span>,梯度<span class="math inline">\(g(w)=\Delta f(w)\)</span>,精度要求<span class="math inline">\(\epsilon\)</span></p><p><em>output</em>:最优参数值<span class="math inline">\(w^*\)</span>；最优模型<span class="math inline">\(P_w\cdot(y|x)\)</span></p><ol type="1"><li>选定初始点<span class="math inline">\(w^{(0)}\)</span>，取<span class="math inline">\(B_0\)</span>为正定对称矩阵，置<span class="math inline">\(k=0\)</span>;</li><li>计算<span class="math inline">\(g_k=g(w^{(k)})\)</span>,若<span class="math inline">\(||g_k||&lt;\epsilon\)</span>,则停止计算，得<span class="math inline">\(w^*=w^{(k)}\)</span>,否则跳转3；</li><li>由<span class="math inline">\(B_kp_k=-g_k\)</span>求出<span class="math inline">\(p_k\)</span>;</li><li>一维搜索：求<span class="math inline">\(\lambda_k\)</span>使得</li></ol><p><span class="math display">\[f(w^{k}+\lambda_kp_k)=\min_{\lambda\geq0}f(w^{(k)}+\lambda p_k)\]</span></p><ol start="5" type="1"><li>置<span class="math inline">\(w^{(k+1)}=w^{(k)}+\lambda_kp_k\)</span>;</li><li>计算<span class="math inline">\(g_{k+1} = g(w^{(k+1)})\)</span>,若<span class="math inline">\(||g_k||&lt;\epsilon\)</span>,则停止计算，得<span class="math inline">\(w^*=w^{(k)}\)</span>,否则求出<span class="math inline">\(B_{k+1}\)</span></li></ol><p><span class="math display">\[B_{k+1}=B_k+\frac{y_ky_k^T}{y^T_k\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta^T_kB_k\delta_k}\\其中，y_k=g_{k+1}-g_k,\delta_k=w^{(k+1)}-w^{(k)}\]</span></p><ol start="7" type="1"><li>置<span class="math inline">\(k=k+1\)</span>,转至3；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k近邻法</title>
      <link href="/archives/dff65b00.html"/>
      <url>/archives/dff65b00.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>k近邻算法：</p><p>给定的一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最的k个实例，这k个实例的多数属于某个类，就将该实例划分进入这个类中；(哲学上讲的话，有点近朱者赤近墨者黑的味道~~)</p></blockquote><h2 id="算法核心原理">算法核心原理</h2><ul><li><p>根据给定的<strong>距离度量</strong>，在训练数据集<em>T</em>中找出与<em>x</em>最近邻的<em>k</em>个点，涵盖这<em>k</em>个点的邻域记为</p><p><span class="math inline">\(N_{k}(x)\)</span>;</p></li><li><p>在<span class="math inline">\(N_{k}(x)\)</span>中根据<strong>分类决策规则</strong>决定<span class="math inline">\(x\)</span>的类别<span class="math inline">\(Y\)</span>; <span class="math display">\[Y= arg\max \sum_{x_i\in N_{k}(x)}I(y_{i}=c_{j}),i=1,2 \ldots,N;j=1,2 \ldots,K;\\其中I为指示函数；即y_i=c_j时I为1，否则为I为0；\]</span> <strong><em>k近邻没有显示的学习过程</em></strong></p></li></ul><h2 id="k近邻模型">k近邻模型</h2><p>k近邻算法使用的模型实际上 对应于特征空间的划分；模型有三个基本要</p><ul><li>度量距离</li><li>k值的选择</li><li>分类决策的规则</li></ul><h3 id="距离度量">距离度量</h3><p>将设特征空间<span class="math inline">\(X\)</span>是<span class="math inline">\(n\)</span>维实数向量空间<span class="math inline">\(R^n\)</span>,<span class="math inline">\(x_i,x_j\in X,x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T,x_i,x_j的L_p距离定义为\)</span> <span class="math display">\[L_p(x_I,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|)^{\frac1p}\]</span> 其中<span class="math inline">\(P\geq 1\)</span></p><ul><li><span class="math inline">\(p=1\)</span>称为曼哈顿距离</li><li><span class="math inline">\(p=2\)</span>称为欧氏距离</li><li><span class="math inline">\(p=\infty\)</span>时，他是各个坐标距离的最大值</li></ul><p>如图</p><p><img src="/archives/Lp距离间的关系.png"></p><h3 id="k值的选择">k值的选择</h3><ul><li><p>k值小，预测结果对近邻实例点非常敏感，而且k值减小模型就会变得复杂而且容易发生过拟合；</p></li><li><p>k值大，学习的近似误差会增大，也就意味这模型会变得简单；</p><p><strong>通常采用交叉验证发来选取最优k值</strong></p></li></ul><h3 id="分类决策规则">分类决策规则</h3><p>k近邻法的分类决策规则往往是多数表决；</p><p>如果分类损失函数为0-1损失函数，分类函数为 <span class="math display">\[f:R^n \to \{c_1,c_2,\cdots,c_k\}\]</span> 那么误分类的概率 <span class="math display">\[P(Y\neq f(X)) = 1-P(Y=f(X))\]</span> 对于给定的实例<span class="math inline">\(x\in X\)</span>,其最近邻的<span class="math inline">\(k\)</span>个训练实例点构成的集合<span class="math inline">\(N_k(x)\)</span>,如果覆盖<span class="math inline">\(N_k(x)\)</span>的区域类别为<span class="math inline">\(c_j\)</span>,那么误分类 <span class="math display">\[\frac1k\sum_{x_i\in N_k(x)} = 1-\frac1k\sum_{x_i\in N_k(x)}I(y_i=c_j)\]</span> 使得误分类概率最小就是经验风险最小，也就是<span class="math inline">\(\frac1k\sum_{x_i\in N_k(x)}I(y_i=c_j)\)</span>最大；也就是说多数表决规则等价于经验风险最小化；</p><h2 id="k近邻算法实现kd树">K近邻算法实现：kd树</h2><p>由于实现k近邻算法要对已划分数据进行搜算，数据量大时会耗费大量时间，因此有很多优化提高效率的算法；dk树就是其中的一种</p><h3 id="构造kd树">构造kd树</h3><ol type="1"><li>构造根节点，根节点对应于包含<span class="math inline">\(T\)</span>的k维空间的超矩形区域；</li><li>选择<span class="math inline">\(x^{(1)}\)</span>为坐标轴，以<span class="math inline">\(T\)</span>中所有实例的<span class="math inline">\(x^{(1)}\)</span>坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域；切分由通过切分点并与坐标轴<span class="math inline">\(x^{(1)}\)</span>垂直的超平面实现；左子节点存放对应坐标小于切分点的点、右边存放大于切分点的，根节点保存出与切分平面上的点；</li><li>重复：对于深度为j的节点，选择<span class="math inline">\(x^{(l)}\)</span>为切分的坐标轴，<span class="math inline">\(l=j(mod\,k)+1\)</span>,以该节点的区域中所有实例的<span class="math inline">\(x^{(l)}\)</span>坐标的中位数为切分点，再次进行与（2）中类似的划分；</li><li>直到两个子区域没有实例存在时停止，从而形成<span class="math inline">\(kd\)</span>树的区域划分；</li></ol><p>图示如下</p><p><img src="/archives/kd树示例.png"></p><h3 id="搜索kd树">搜索kd树</h3><p>包含目标点的叶节点对应的包含目标点的最小超矩形区域。以此叶节点的实例点作为当前最近点。目标点的最邻近点一定在以目标点为中心并通过当前最近点的超群体内部。然后返回当前节点的父节点，如果父节点的另一子节点的超矩形区域与超球体相交，那么在相交区域内寻找与目标结点最近的点。如果存在，将此作为新的最近点。算法转到更上一级父节点继续上述过程；如果父节点另一子节点的超矩形区域与超球体不相交，或不存在比当前最近点更近点则停止操作；</p><ol type="1"><li>在kd树中找出包含目标点x的叶节点：从根节点出发递归的向下方位，直到子节点为叶节点为止；</li><li>以此叶节点为“当前最近点”；</li><li>递归的向上回退，在每个节点执行下述操作<ul><li>如果该节点保存的实例点比当前最近点距离目标更近，则该实例点为“当前最近点”；</li><li>当前最近点一定位于该节点一个子节点对应的区域，检查该子节点的父节点的另一个子节点对应的区域是否有更近的点，有则确认零一点为最近点----具体可检查另一子节点对应区域是否以目标点为球心、以当前最小距离为半径的超球体相交；</li><li>不相交则向上回退；</li></ul></li><li>回退到根节点，搜索结束；</li></ol><p>图示如下</p><p><img src="/archives/kd树搜索.png"></p><p><strong><span class="math inline">\(kd\)</span>树搜索平均计算复杂度是<span class="math inline">\(O(log\,N)\)</span>,<span class="math inline">\(N\)</span>为训练实例数。kd树适合用于训练实例数远大于空间维数时的k紧邻搜索。二者接近时，<span class="math inline">\(kd\)</span>树效率会迅速下降，几乎线性扫描；</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/archives/8ddc7426.html"/>
      <url>/archives/8ddc7426.html</url>
      
        <content type="html"><![CDATA[<h2 id="核心原理">核心原理</h2><blockquote><p>可以将决策树看成是一个if-then规则的集合，决策树的路径或其对应的if-then规则集合<strong>互斥且完备</strong>；每一个实例都被一条路径或一条规则覆盖，而且只被一条路径或规则覆盖；</p></blockquote><p>决策树与条件概率分布示意图</p><p><img src="/archives/决策树对应条件概率分布.png"></p><p><strong>决策树学习的目标</strong></p><p>根据给定的训练数据集构建一个决策树模型（学习），并使他们能够对实例进行正确的分类(泛 化)；</p><p><strong>决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化</strong>；损失函数确定后，学习问题就变成了损失函数意义下选择最优决策树的问题；由于从所有决策树中寻找最优决策树是np完全问题，所以常常使用启发式的算法来近似求解这一最优化问题；以此寻找到次最优的；</p><hr><p><strong>决策树学习的核心</strong></p><ul><li>特征选择——（选取核心数据）</li><li>决策树生成——（构建决策树）——局部最优</li><li>决策树剪枝——（提高泛化能力）——全局最优</li></ul><h2 id="特征选择">特征选择</h2><h2 id="信息增益">信息增益</h2><p><strong>信息增益是衡量特征选择的一个重要指标；</strong>用表示训练数据集对应某个特征所具备的分类能力；</p><h3 id="熵">熵</h3><blockquote><p>信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为 <span class="math display">\[P(X=x_i)=p_i,i=1,2,\cdots,n\]</span> 则随机变量的熵定义为 <span class="math display">\[H(X)=-\sum\limits^n_{i=1}p_i\log p_i\\其中定义：0\log0=0\]</span> 其中对数常取2为底或者e为底，这时熵的单位分别是比特或纳特。从定义可知熵只依赖于X的分布，而与X的取值无关，所以也将X的熵就做<span class="math inline">\(H(p)\)</span>;熵越大，随机变量的不确定性就越大，且有定义可知 <span class="math display">\[0\leq H(p)\leq\log n（基本不等式可推导）\]</span> p_当随机变量只有两个取值0，1时，熵为 <span class="math display">\[H(p)=-p\log_2p-(1-p)\log_2(1-p)\]</span> 这时熵<span class="math inline">\(H(p)\)</span>随概率<span class="math inline">\(p\)</span>变化的曲线如图所示</p><p><img src="/archives/熵与概率变化的曲线.png"></p><p>条件熵<span class="math inline">\(H(Y|X)\)</span>表示在随机变量X已知的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望 <span class="math display">\[H(Y|X)=\sum\limits_{i=1}^np_iH(Y|X=x_i),其中p_i=P(X=x_i),i=1,2,\cdots,n\]</span></p><hr><p>当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验上和经验条件熵</p></blockquote><p><strong>信息增益表示得知特征X的信息儿时的类Y的信息不确定性减少的程度</strong></p><p>定义：特征A对训练数据集D的信息增益g(D,A),集合D的经验熵<span class="math inline">\(H(D)\)</span>与特征A给定条件下D的经验条件熵<span class="math inline">\(H(D|A)\)</span>之差 <span class="math display">\[g(D,A)=H(D)-H(D|A)\]</span></p><h3 id="算法">算法</h3><ol type="1"><li>计算数据集D的经验熵<span class="math inline">\(H(D)\)</span></li></ol><p><span class="math display">\[H(D)=-\sum\limits_{k=1}^k\frac{|C_K|}{|D|}\log_2\frac{|C_K|}{|D|}\]</span></p><ol start="2" type="1"><li><p>计算特征A对数据集D的经验条件熵 <span class="math display">\[H(D|A)= \sum^n_{i=1}\frac{|D_i|}{|D|}H(D_i)\]</span></p></li><li><p>计算信息增益</p></li></ol><p><span class="math display">\[g(D,A) = H(D)-H(D|A)\]</span></p><h3 id="信息增益比">信息增益比</h3><p>信息增益划分数据及特征存在偏向于选择取值较多的特征的问题。使用信息增益比可以对此作出矫正</p><p>信息增益比定义： <span class="math display">\[g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\其中，H_A(D)=-\sum^n_{i=1}\frac{|D_i|}{|D|}\log_2\frac{|Di|}{|D|}\]</span></p><h2 id="决策树生成">决策树生成</h2><h4 id="id3算法">ID3算法</h4><ol type="1"><li>若D中所有的实例属于同一类，则T为单节点树，并将类<span class="math inline">\(C_k\)</span>作为该节点的类标记，返回T;</li><li>若<span class="math inline">\(A=\emptyset\)</span>,则T为单节点树，并将D中的实例树最大的类<span class="math inline">\(C_K\)</span>作为该节点的类标记，返回T;</li><li>否则，计算A中各特征对D的信息增益，选择信息增益最大的特征<span class="math inline">\(A_g\)</span>;</li><li>如果<span class="math inline">\(A_g\)</span>的信息增益小于阀值，则置T为单节点树，并将D中实例数最大的类<span class="math inline">\(C_k\)</span>作为标记，返回T；</li><li>否则，对于<span class="math inline">\(A_g\)</span>中的每一可能值<span class="math inline">\(a_i\)</span>,依<span class="math inline">\(A_g=a_i\)</span>,将D分割为若干非空子集<span class="math inline">\(D_i\)</span>，将<span class="math inline">\(D_I\)</span>中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T;</li><li>对第i个子节点，以<span class="math inline">\(D_i\)</span>为训练集，以<span class="math inline">\(A-\{A_g\}\)</span>为特征集，递归的调用步骤1~5，得到子树<span class="math inline">\(T_i\)</span>,返回<span class="math inline">\(T_i\)</span>;</li></ol><h4 id="c4.5的生成算法">C4.5的生成算法</h4><p>与ID3算法相似，C4.5在生成的过程中，用信息增益比来选择特征；</p><h2 id="决策树的剪枝">决策树的剪枝</h2><h3 id="原理">原理</h3><p>递归生成的决策树往往对训练数据分类准确但是对未知数据的分类没有那么准确，会出现过拟合现象，因此需要从已经生成的树上裁掉一些子树或叶节点，从而简化分类树模型；</p><p><strong>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现；</strong>设树T的叶节点个数为<span class="math inline">\(|T|\)</span>,t是树T的叶节点，该叶节点有<span class="math inline">\(N_t\)</span>个样本点，其中k类的样本点有<span class="math inline">\(N_{tk}\)</span>个，<span class="math inline">\(k=1,2,\cdots,K,H_t(T)\)</span>为叶节点上的经验熵，<span class="math inline">\(\alpha\geq0\)</span>为参数，则决策树损失函数定义为 <span class="math display">\[C_a(T)=\sum\limits^{|T|}_{t=1}{N_tH_t(T)+\alpha|T|}\\其中经验熵为:H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\\简写为：C_\alpha(T)=C(T)+\alpha|T|\]</span> <span class="math inline">\(C(T)\)</span>表示模型对讯联数据的预测误差，|T|表示模型复杂程度，<span class="math inline">\(\alpha\)</span>用来控制模型复杂度和准确度之间的影响；</p><h3 id="算法-1">算法</h3><ol type="1"><li>计算出每个节点的经验熵；</li><li>递归的从树的叶节点向上回缩；设一组叶节点回缩到其父节点之前与之后的整体树分别为<span class="math inline">\(T_B\)</span>与<span class="math inline">\(T_A\)</span>,其对应的损失函数值分别为<span class="math inline">\(C_a(T_B)\)</span>与<span class="math inline">\(C_a(T_A)\)</span>，如果<span class="math inline">\(C_a(T_A)\leq C_a(T_B)\)</span>则进行剪枝，将父节点变为新的叶节点；</li><li>返回2，知道不能继续为止，得到损失函数最小的子树<span class="math inline">\(T_\alpha\)</span>;</li></ol><p><strong>注意，只需要考虑两个数的损失函数的差，其计算可以再局部进行。所以决策树的剪枝算法可以由一种动态规划算法来实现</strong></p><h2 id="cart算法">CART算法</h2><h3 id="crat生成">CRAT生成</h3><h4 id="回归树">回归树</h4><p>递归的构建二叉树，对回归树用平方误差最小化准则，对分类树使用基尼指数最小化准则，生成二叉树；</p><p>定义两个区域： <span class="math display">\[R_1(j,s)=\{x|x^{(j)}\leq s\}，R_2(j,s)=\{x|x^{(j)}&gt; s\}，\]</span> 寻找最优切分变量<span class="math inline">\(j\)</span>和最优切分点<span class="math inline">\(s\)</span>,即求解 <span class="math display">\[\min\limits_{j,s}[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}{(y_i-c_1)^2} +\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}{(y_i-c_2)^2} ]\]</span> 对固定的输入变量<span class="math inline">\(j\)</span>可以找到最优切分点<span class="math inline">\(s\)</span> <span class="math display">\[\hat c_1 = ave(y_i|x_i\in R_1(j,s)),\hat c_2 = ave(y_i|x_i\in R_2(j,s))\]</span> 遍历所有输入变量，找到切分点构成一对<span class="math inline">\((j,s)\)</span>.依此将输入空间划分成两个区域，紧接着重复划分，到满足停止条件；这样就得到了一棵回归树，这样的称为最小二乘回归树；</p><p><strong>算法</strong></p><ol type="1"><li>选择最优切分变量<span class="math inline">\(j\)</span>与切分点<span class="math inline">\(s\)</span>,求解</li></ol><p><span class="math display">\[\min\limits_{j,s}[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}{(y_i-c_1)^2} +\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}{(y_i-c_2)^2} ]\]</span></p><p>找到使之最小的一组解<span class="math inline">\((j,s)\)</span></p><ol start="2" type="1"><li>用选定的对<span class="math inline">\((j,s)\)</span>划分区域并决定对应的输出值：</li></ol><p><span class="math display">\[\hat c_m=\frac1{N_m}\sum\limits_{s_i\in R_m(j,x)}y_i,\,x\in R_m,\,\,m=1,2\]</span></p><ol start="3" type="1"><li>继续对两个区域调用步骤1,2，直至满足停止条件</li><li>将输入控件划分成<span class="math inline">\(M\)</span>个区域，生成决策树；</li></ol><p><span class="math display">\[f(x)=\sum\limits^M_{m=1}\hat c_mI(x\in R_m)\]</span></p><h4 id="分类树">分类树</h4><p><strong>基尼指数定义</strong></p><p>分类问题中，假设有k个类，样本点属于第k类的概率为概率为<span class="math inline">\(p_k\)</span>,对于给定的样本集合<span class="math inline">\(D\)</span>,其基尼指数为 <span class="math display">\[Gini(p)=1-\sum\limits^K_{k=1}p^2_k\]</span> 在特征A的条件下，集合D的基尼指数定义为 <span class="math display">\[Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\]</span> 基尼指数<span class="math inline">\(Gini(D)\)</span>表示集合<span class="math inline">\(D\)</span>的不确定性，基尼指数<span class="math inline">\(Gini(D,A)\)</span>表示经过类<span class="math inline">\(A=a\)</span>分割后集合<span class="math inline">\(D\)</span>的不确定性；基尼指数越大，不确定性就越大</p><p><img src="/archives/基尼指数_熵_分类误差率.png"></p><p><strong>算法</strong></p><ol type="1"><li><p>设节点的训练数据集为<span class="math inline">\(D\)</span>,计算现有特征对该数据及的基尼指数。对每一个特征<span class="math inline">\(A\)</span>,其可能的每个取值<span class="math inline">\(a\)</span>根据样本点对<span class="math inline">\(A=a\)</span>测试为“是”或”否“将<span class="math inline">\(D\)</span>分割为<span class="math inline">\(D_1,D_2\)</span>两部分,计算<span class="math inline">\(A=a\)</span>时<span class="math inline">\(Gini(D,A)\)</span>；</p></li><li><p>在所有可能的特征<span class="math inline">\(A\)</span>以及它们所有可能的切分点<span class="math inline">\(a\)</span>中选择基尼指数最小的特征及其切分点；并生成两个子节点。将现有的数据集依据特征分配到两个子节点中；</p></li><li><p>对两个子节点递归的调用1,2;</p></li><li><p>生成CART决策树；</p></li></ol><h3 id="cart剪枝">CART剪枝</h3><p><strong>原理</strong></p><p>定义子树的损失函数： <span class="math display">\[C_\alpha(T)=C(T)+\alpha|T|\]</span> 具体的，从整体<span class="math inline">\(T_0\)</span>,开始剪枝，对<span class="math inline">\(T_0\)</span>的任意内部节点<span class="math inline">\(t\)</span>,以<span class="math inline">\(t\)</span>为单节点树的损失函数为 <span class="math display">\[C_\alpha(t) = C(t)+\alpha\]</span> 以<span class="math inline">\(t\)</span>为根节点的子树<span class="math inline">\(T_t\)</span>的损失函数为 <span class="math display">\[C_\alpha(T_t)=C(T_t)+\alpha|T_t|\]</span> 通过上面式易得，<span class="math inline">\(\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}\)</span>为剪枝的零界点，此时虽然二者有相同的损失函数，但<span class="math inline">\(t\)</span>比<span class="math inline">\(T_t\)</span>结点少，因此应该剪枝；</p><p>为此对<span class="math inline">\(T_0\)</span>中的每一个内部节点<span class="math inline">\(t\)</span>计算 <span class="math display">\[g(t)=\frac{C(t)-C(T_t)}{|T_t-1|}\]</span> 表示剪枝后整体损失函数减小的程度.在<span class="math inline">\(T_0\)</span>中减去<span class="math inline">\(g(t)\)</span>最小的<span class="math inline">\(T_t\)</span>,将得到的子树作为<span class="math inline">\(T_t\)</span>,将得到的子树作为<span class="math inline">\(T_1\)</span>，同时将最小的<span class="math inline">\(g(t)\)</span>设为<span class="math inline">\(\alpha_1\)</span>.<span class="math inline">\(T_1\)</span>为区间<span class="math inline">\([\alpha_1,\alpha_2)\)</span>的最优子树。如此不断剪枝直到根节点；期间不断产生新的<span class="math inline">\(\alpha\)</span>,并产生新的区间；</p><p><strong>在得到的子树序列中使用交叉验证发求得最优子树<span class="math inline">\(T_\alpha\)</span></strong></p><p>具体的，利用独立的验证数据集，测试子树序列中的各个子树的平方误差或基尼指数。寻找对应指数最小的子树，同时子树确定也就意味着独影的<span class="math inline">\(\alpha_k\)</span>确定；</p><hr><p><strong>算法</strong></p><ol type="1"><li><p>设<span class="math inline">\(k=0,T=T_0\)</span>;</p></li><li><p>设<span class="math inline">\(\alpha=+\infty;\)</span></p></li><li><p>自上而下的对各内部节点<span class="math inline">\(t\)</span>计算<span class="math inline">\(C(T_1),|T_t|\)</span>以及</p></li></ol><p><span class="math display">\[g(t)=\frac{C(t)-C(T_t)}{|T_t|-1} \,,\alpha=\min(\alpha,g(t))\]</span></p><p>这里，<span class="math inline">\(T_t\)</span>表示以<span class="math inline">\(t\)</span>为根节点的子树，<span class="math inline">\(C(T_t)\)</span>是对训练数据集的预测误差，<span class="math inline">\(|T_t|\)</span>是<span class="math inline">\(T_t\)</span>的叶节点个数；</p><ol start="4" type="1"><li>对<span class="math inline">\(g(t)=\alpha\)</span>的内部节点<span class="math inline">\(t\)</span>进行剪枝，并对叶节点<span class="math inline">\(t\)</span>以多数表决的方法确定其类，得到树<span class="math inline">\(T\)</span>;</li><li>设<span class="math inline">\(k=k+1,\alpha_k=\alpha,T_k=T\)</span>;</li><li>如果<span class="math inline">\(T_k\)</span>不是由根节点及两个叶节点构成的树，则回退到步骤2；否则令<span class="math inline">\(T_k=T_n\)</span>;</li><li>采用交叉验证法在子树序列<span class="math inline">\(T_0,T_1,\cdots,T_n\)</span>中选取最优子树<span class="math inline">\(T_\alpha\)</span>;</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯</title>
      <link href="/archives/f77f0066.html"/>
      <url>/archives/f77f0066.html</url>
      
        <content type="html"><![CDATA[<h2 id="数学基础">数学基础</h2><p>先验概率： 根据以往的经验分析得到的概率；</p><p>后验概率：根据结果推导原因；</p><p>贝叶斯公式：</p><p><img src="/archives/贝叶斯公式.png"></p><h2 id="核心原理">核心原理</h2><p>朴素贝叶斯法通过训练数据集学习联合概率分布<span class="math inline">\(P(X,Y)\)</span>，具体的学习先验概率分布 <span class="math display">\[P(Y=c_k),k=1,2,\cdots,K\]</span> 条件概率分布 <span class="math display">\[P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\cdots,K\]</span> 于是学习到联合概率分布<span class="math inline">\(P(X,Y)\)</span></p><p>实际上，朴素贝叶斯对条件概率分布做了<strong>条件独立性的假设</strong>，这一假设使得朴素贝叶斯发相对简单；但也会<strong>牺牲一定的分类准确率</strong>；</p><hr><p>朴素贝叶斯法分类时，对于给定的输入x，通过学习到的模型计算后验概率分布，将后验概率最大的类作为x的输出类。具体的朴素贝叶斯分类器可以表示为 <span class="math display">\[y=f(x)=arg\max\limits_{c_k} \frac{P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}\]</span> 由于上式中所有分母都是相同的，因此上式等价于 <span class="math display">\[y=arg\max \limits_{c_k} P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)\]</span></p><hr><p><strong>后验概率最大化等价于经验风险最小化</strong>（后验概率最大化的含义）</p><p><img src="/archives/后验概率最大化含义.png"></p><h2 id="朴素贝叶斯的参数估计">朴素贝叶斯的参数估计</h2><h3 id="极大似然估计">极大似然估计</h3><ol type="1"><li><p>计算先验概率及条件概率 <span class="math display">\[P(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)}{N},k=1,2,\cdots,K\]</span></p><p><span class="math display">\[P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_j=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}\\j=1,2,\cdots,n;l=1,2,\cdots,S_j;k=1,2,\cdots,K\]</span></p></li><li><p>对于给定的实例<span class="math inline">\(x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T\)</span>,计算 <span class="math display">\[p(Y=c_k)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,\cdots,K\]</span></p></li><li><p>确定实例x的类 <span class="math display">\[y=arg\max\limits_{c_k}P(Y=c_k)\prod\limits^n_{j=1}P(X^{j}=x^{j}|Y=c_k)\]</span></p></li></ol><h3 id="贝叶斯估计">贝叶斯估计</h3><p>极大似然估计可能会出现要估计的概率值为0的情况，这会影响到后验概率的计算结果。使分类产生偏差。可以用贝叶斯估计来解决这一问题；具体的</p><hr><p>先验概率的贝叶斯估计是 <span class="math display">\[P_\lambda(Y=c_k)=\frac{\sum\limits_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}\]</span> 条件概率的贝叶斯估计是 <span class="math display">\[P\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum\limits^{S_j}_{i=1}I(y_i=c_k)+S_j\lambda}\]</span></p><hr><p>其中，<span class="math inline">\(\lambda\geq0\)</span>,相当于在随机变量各个取值上赋予一个整数<span class="math inline">\(\lambda&gt;0\)</span>.当<span class="math inline">\(\lambda=0\)</span>时就是极大似然估计；常取<span class="math inline">\(\lambda=1\)</span>,这时称为拉普拉斯平滑。显然 <span class="math display">\[P_\lambda(X^{(j)}=a_{jl}|Y=c_k)&gt;0\\ \sum\limits^{S_j}_{j=1}P(X^{(j)}=a_{jl}|Y=c_k)=1\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 统计学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
